import streamlit as st
import os
import shutil
import json
import sys
sys.path.append(os.path.dirname(__file__))
from llm_agent import detect_task
st.set_page_config(page_title="LLM-Augmented AutoML", layout="wide")
import pandas as pd

def TT(zh: str, en: str):
	return en if st.session_state.get("lang", "zh") == "en" else zh

# è¯­è¨€é€‰æ‹©ï¼ˆé»˜è®¤ä¸­æ–‡ï¼‰
if "lang" not in st.session_state:
	st.session_state["lang"] = "zh"
_lang_choice = st.sidebar.selectbox(
	"Language / è¯­è¨€",
	["ä¸­æ–‡", "English"],
	index=0 if st.session_state.get("lang", "zh") == "zh" else 1,
)
st.session_state["lang"] = "zh" if _lang_choice == "ä¸­æ–‡" else "en"

st.title(TT("LLM-Augmented AutoML (æœ¬åœ°è®­ç»ƒ)", "LLM-Augmented AutoML (Local Training)"))
st.success(TT("ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸã€‚æ¥ä¸‹æ¥å°†å®ç°æ•°æ®ä¸Šä¼ ã€åˆ¤å®šä¸æŠ¥å‘Šã€‚", "Environment initialized. Next: upload, detection, and reporting."))

# ä¼šè¯é¦–æ¬¡è¿è¡Œæ—¶æ¸…ç©º examples ç›®å½•ï¼ˆé¿å…æ¯æ¬¡ rerun éƒ½æ¸…ç©ºæ–°ä¸Šä¼ æ–‡ä»¶ï¼‰
try:
	if not st.session_state.get("__examples_cleared__", False):
		project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
		examples_dir = os.path.join(project_root, 'examples')
		if os.path.exists(examples_dir):
			for name in os.listdir(examples_dir):
				p = os.path.join(examples_dir, name)
				try:
					if os.path.isfile(p) or os.path.islink(p):
						os.remove(p)
					elif os.path.isdir(p):
						shutil.rmtree(p)
				except Exception:
					# å•ä¸ªæ–‡ä»¶å¤±è´¥ä¸å½±å“æ•´ä½“
					pass
		st.session_state["__examples_cleared__"] = True
		st.caption(TT("å·²æŒ‰éœ€æ±‚æ¸…ç©º examples æ–‡ä»¶å¤¹ï¼ˆæœ¬ä¼šè¯ä»…ä¸€æ¬¡ï¼‰ã€‚", "Cleared examples folder as requested (once per session)."))
except Exception as __clear_err:
	st.warning(TT(f"æ¸…ç©º examples æ–‡ä»¶å¤¹å¤±è´¥ï¼š{__clear_err}", f"Failed to clear examples folder: {__clear_err}"))

"""å¤šæ–‡ä»¶ä¸Šä¼ åŒº"""
uploaded_files = st.file_uploader(TT("ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ª CSV æ–‡ä»¶", "Upload one or more CSV files"), type=["csv"], accept_multiple_files=True)

active_df = None
df_source_name = None
loaded_dfs = {}

# ç»Ÿä¸€åˆ›å»ºç”¨äºä¸Šæ–¹é€‰æ‹©ä¸é¢„è§ˆçš„å®¹å™¨ï¼Œé¿å…åˆ†æ”¯æœªå®šä¹‰
select_container = st.container()
preview_container = st.container()

if uploaded_files:
	examples_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'examples')
	os.makedirs(examples_dir, exist_ok=True)
	# ä»…åŠ è½½ä¸ä¿å­˜æ–‡ä»¶ï¼Œä¸åœ¨å¾ªç¯ä¸­æ¸²æŸ“å…¬å…±åˆ—ç›¸å…³ç»„ä»¶ï¼Œé¿å…é‡å¤ key
	for uf in uploaded_files:
		try:
			df_tmp = pd.read_csv(uf)
			loaded_dfs[uf.name] = df_tmp
			save_path = os.path.join(examples_dir, uf.name)
			with open(save_path, 'wb') as f:
				f.write(uf.getbuffer())
		except Exception as e:
			st.error(TT(f"è¯»å–æ–‡ä»¶ {uf.name} å¤±è´¥: {e}", f"Failed to read file {uf.name}: {e}"))

	# è®¡ç®—å¤šæ–‡ä»¶å…¬å…±åˆ—ï¼ˆä¸€æ¬¡æ€§ï¼‰
	if len(loaded_dfs) >= 2:
		list_of_colsets = [set(df.columns) for df in loaded_dfs.values() if hasattr(df, 'columns')]
		if list_of_colsets:
			common_columns = set.intersection(*list_of_colsets) if len(list_of_colsets) > 1 else list_of_colsets[0]
		else:
			common_columns = set()
	else:
		common_columns = set()

	# é¡¶éƒ¨å®¹å™¨ï¼ˆå•å®ä¾‹ï¼‰
	select_container = st.container()
	preview_container = st.container()

	with st.expander(TT("ğŸ“Œ å¤šæ–‡ä»¶å…¬å…±åˆ— (æ‰€æœ‰æ–‡ä»¶éƒ½åŒ…å«)", "ğŸ“Œ Common columns across all files"), expanded=True):
		if common_columns:
			st.write(TT(f"å…± {len(common_columns)} ä¸ªå…¬å…±åˆ—ï¼š", f"Total {len(common_columns)} common column(s):"))
			st.code("\n".join(sorted(common_columns)))
		else:
			st.warning(TT("æœªæ‰¾åˆ°æ‰€æœ‰æ–‡ä»¶éƒ½å…±åŒæ‹¥æœ‰çš„åˆ—ã€‚", "No columns are common to all files."))

		# å¯é€‰å±•ç¤ºæ¯ä¸ªæ–‡ä»¶ç¼ºå¤±å…¬å…±åˆ—æƒ…å†µï¼ˆå•å®ä¾‹æ§ä»¶ï¼Œé¿å…é‡å¤ keyï¼‰
		show_diff = st.checkbox(TT("æ˜¾ç¤ºå„æ–‡ä»¶ç¼ºå¤±å…¬å…±åˆ—æƒ…å†µ", "Show missing common columns per file"), value=False, key="show_diff_missing_cols")
		if show_diff and common_columns:
			for fname, df_tmp in loaded_dfs.items():
				missing_in_file = common_columns - set(df_tmp.columns)
				if missing_in_file:
					st.error(TT(f"{fname} ç¼ºå¤± {len(missing_in_file)} ä¸ªå…¬å…±åˆ—ï¼š{', '.join(sorted(missing_in_file))}", f"{fname} missing {len(missing_in_file)} common column(s): {', '.join(sorted(missing_in_file))}"))
				else:
					st.success(TT(f"{fname} åŒ…å«å…¨éƒ¨å…¬å…±åˆ— âœ”", f"{fname} includes all common columns âœ”"))

		# ---------------- åˆå¹¶åŠŸèƒ½ï¼ˆæ¨ªå‘ / çºµå‘ï¼‰ ----------------
		st.markdown("---")
		st.markdown(TT("### ğŸ”— åˆå¹¶å·¥å…·", "### ğŸ”— Merge Tool"))
		merge_mode = st.radio(
			TT("é€‰æ‹©åˆå¹¶æ–¹å¼", "Choose merge mode"),
			[TT("çºµå‘å †å ï¼ˆä»…å…¬å…±åˆ—ï¼‰", "Vertical stack (common columns only)"), TT("æ¨ªå‘åŒ¹é…ï¼ˆå…¬å…±åˆ—ä½œä¸ºé”®ï¼Œåˆå¹¶å…¶ä½™åˆ—ï¼‰", "Horizontal join (use common columns as keys)")],
			index=0,
			help=TT("æ¨ªå‘åŒ¹é…=ç±»ä¼¼å¤šè¡¨ joinï¼›çºµå‘å †å =append è¡Œã€‚", "Horizontal join ~ multi-table join; vertical stack ~ append rows.")
		)

		if merge_mode.startswith(TT("çºµå‘", "Vertical")):
			st.caption(TT("ä»…ä¿ç•™å…¬å…±åˆ—å¹¶æŒ‰è¡Œå †å ï¼ˆä¹‹å‰çš„è¡Œä¸ºï¼‰ã€‚", "Keep only common columns and stack rows (previous behavior)."))
			add_source_col = st.checkbox(TT("æ·»åŠ æ¥æºæ–‡ä»¶åˆ— (_source_file)", "Add source file column (_source_file)"), value=True, key="add_source_file")
			merge_btn = st.button(TT("âš™ï¸ æ‰§è¡Œçºµå‘åˆå¹¶", "âš™ï¸ Run vertical merge"), key="merge_vertical")
			if merge_btn:
				if not common_columns:
					st.error(TT("æ— æ³•åˆå¹¶ï¼šæ²¡æœ‰å…¬å…±åˆ—ã€‚", "Cannot merge: no common columns."))
				else:
					try:
						merged_parts = []
						for fname, df_part in loaded_dfs.items():
							subset = df_part[list(common_columns)].copy()
							if add_source_col:
								subset["_source_file"] = fname
							merged_parts.append(subset)
						merged_df = pd.concat(merged_parts, ignore_index=True)
						base_name = "merged_common.csv"
						final_name = base_name
						idx = 1
						while final_name in loaded_dfs:
							idx += 1
							final_name = f"merged_common_{idx}.csv"
						try:
							examples_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'examples')
							os.makedirs(examples_dir, exist_ok=True)
							merged_path = os.path.join(examples_dir, final_name)
							merged_df.to_csv(merged_path, index=False, encoding="utf-8")
						except Exception as fs_err:
							st.warning(TT(f"åˆå¹¶æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼Œä½†å†…å­˜ä¾æ—§å¯ç”¨ï¼š{fs_err}", f"Failed to save merged file, but in-memory data is available: {fs_err}"))
						loaded_dfs[final_name] = merged_df
						st.session_state["preferred_file_name"] = final_name
						st.session_state["merged_common_df"] = merged_df
						st.success(TT(f"çºµå‘åˆå¹¶æˆåŠŸï¼š{final_name}ï¼Œå½¢çŠ¶ {merged_df.shape}", f"Vertical merge success: {final_name}, shape {merged_df.shape}"))
						csv_bytes = merged_df.to_csv(index=False).encode('utf-8')
						st.download_button(TT("â¬‡ï¸ ä¸‹è½½ç»“æœ", "â¬‡ï¸ Download result"), data=csv_bytes, file_name=final_name, mime="text/csv")
						st.info(TT("åœ¨ä¸Šæ–¹æ–‡ä»¶é€‰æ‹©æ¡†ä¸­å¯é€‰æ‹©è¯¥åˆå¹¶æ–‡ä»¶ç»§ç»­åˆ†æã€‚", "Select this merged file above to continue analysis."))
						st.session_state["train_df"] = merged_df
						st.session_state["train_source_name"] = final_name
						st.success(TT("è®­ç»ƒå°†é»˜è®¤ä½¿ç”¨è¯¥åˆå¹¶ç»“æœã€‚", "Training will default to this merged result."))
					except Exception as merge_err:
						st.error(TT(f"åˆå¹¶å¤±è´¥ï¼š{merge_err}", f"Merge failed: {merge_err}"))

		else:  # æ¨ªå‘åŒ¹é…
			st.caption(TT("ä½¿ç”¨å…¬å…±åˆ—ä½œä¸ºé”®åšå¤šè¡¨ joinï¼Œä¿ç•™æ¯ä¸ªæ–‡ä»¶çš„å…¶ä½™åˆ—ã€‚", "Use common columns as keys to join tables; keep other columns."))
			if not common_columns:
				st.error(TT("æ— æ³•è¿›è¡Œæ¨ªå‘åŒ¹é…ï¼šæ²¡æœ‰å…¬å…±åˆ—ã€‚", "Cannot do horizontal join: no common columns."))
			else:
				key_cols = sorted(common_columns)
				st.info(TT(f"é”®åˆ—ï¼š{', '.join(key_cols)}", f"Key columns: {', '.join(key_cols)}"))
				join_type = st.selectbox(TT("Join ç±»å‹", "Join type"), ["outer", "inner", "left"], index=0, help=TT("outer=ä¿ç•™æ‰€æœ‰é”®; inner=ä»…å…¬å…±é”®; left=ä»¥ç¬¬ä¸€ä¸ªæ–‡ä»¶ä¸ºä¸»è¡¨", "outer=keep all keys; inner=only common keys; left=use first file as left table"))
				prefix_cols = st.checkbox(TT("ä¸ºéé”®åˆ—åŠ æ–‡ä»¶åå‰ç¼€ä»¥é˜²å†²çª", "Prefix non-key columns with filename to avoid conflicts"), value=True, key="prefix_non_key")
				drop_dup = st.checkbox(TT("å¦‚æœæŸæ–‡ä»¶é”®åˆ—æœ‰é‡å¤è¡Œï¼Œä»…ä¿ç•™ç¬¬ä¸€æ¡", "If duplicate keys exist in a file, keep the first"), value=True, key="drop_dup_keys")
				btn_hmerge = st.button(TT("âš™ï¸ æ‰§è¡Œæ¨ªå‘åŒ¹é…åˆå¹¶", "âš™ï¸ Run horizontal join"), key="merge_horizontal")
				if btn_hmerge:
					try:
						merged_df = None
						for idx_file, (fname, df_part) in enumerate(loaded_dfs.items()):
							work_df = df_part.copy()
							missing_keys = [k for k in key_cols if k not in work_df.columns]
							if missing_keys:
								st.error(TT(f"æ–‡ä»¶ {fname} ç¼ºå¤±é”®åˆ— {missing_keys}ï¼Œè·³è¿‡ã€‚", f"File {fname} missing key columns {missing_keys}, skipping."))
								continue
							if drop_dup and work_df.duplicated(subset=key_cols).any():
								dup_count = work_df.duplicated(subset=key_cols).sum()
								st.warning(TT(f"{fname} é”®åˆ—å­˜åœ¨ {dup_count} ä¸ªé‡å¤ï¼Œå°†ä¿ç•™ç¬¬ä¸€æ¡ã€‚", f"{fname} has {dup_count} duplicate key(s); keeping the first."))
								work_df = work_df.drop_duplicates(subset=key_cols, keep='first')
							non_key_cols = [c for c in work_df.columns if c not in key_cols]
							if prefix_cols:
								base_prefix = os.path.splitext(os.path.basename(fname))[0]
								rename_map = {c: f"{base_prefix}__{c}" for c in non_key_cols}
								work_df = work_df.rename(columns=rename_map)
							cols_to_use = key_cols + [c for c in work_df.columns if c not in key_cols]
							if merged_df is None:
								merged_df = work_df[cols_to_use]
							else:
								merged_df = pd.merge(merged_df, work_df[cols_to_use], on=key_cols, how=join_type)
						if merged_df is None:
							st.error(TT("æœªèƒ½ç”Ÿæˆåˆå¹¶ç»“æœï¼ˆå¯èƒ½æ‰€æœ‰æ–‡ä»¶éƒ½è¢«è·³è¿‡ï¼‰", "No merged result produced (perhaps all files were skipped)"))
						else:
							base_name = "merged_horizontal.csv"
							final_name = base_name
							ix = 1
							while final_name in loaded_dfs:
								ix += 1
								final_name = f"merged_horizontal_{ix}.csv"
							try:
								examples_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'examples')
								os.makedirs(examples_dir, exist_ok=True)
								merged_path = os.path.join(examples_dir, final_name)
								merged_df.to_csv(merged_path, index=False, encoding='utf-8')
							except Exception as fs_err:
								st.warning(TT(f"åˆå¹¶æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼Œä½†å†…å­˜ä»å¯ä½¿ç”¨ï¼š{fs_err}", f"Failed to save merged file, but in-memory data is available: {fs_err}"))
							loaded_dfs[final_name] = merged_df
							st.session_state["preferred_file_name"] = final_name
							st.success(TT(f"æ¨ªå‘åŒ¹é…åˆå¹¶æˆåŠŸï¼š{final_name}ï¼Œå½¢çŠ¶ {merged_df.shape}", f"Horizontal join success: {final_name}, shape {merged_df.shape}"))
							csv_bytes = merged_df.to_csv(index=False).encode('utf-8')
							st.download_button(TT("â¬‡ï¸ ä¸‹è½½ç»“æœ", "â¬‡ï¸ Download result"), data=csv_bytes, file_name=final_name, mime="text/csv")
							st.info(TT("åœ¨ä¸Šæ–¹æ–‡ä»¶é€‰æ‹©æ¡†ä¸­å¯é€‰æ‹©è¯¥æ¨ªå‘åˆå¹¶æ–‡ä»¶ç»§ç»­åˆ†æã€‚", "Select this horizontally merged file above to continue analysis."))
							st.session_state["train_df"] = merged_df
							st.session_state["train_source_name"] = final_name
							st.success(TT("è®­ç»ƒå°†é»˜è®¤ä½¿ç”¨è¯¥åˆå¹¶ç»“æœã€‚", "Training will default to this merged result."))
					except Exception as e:
						st.error(TT(f"æ¨ªå‘åˆå¹¶å¤±è´¥ï¼š{e}", f"Horizontal merge failed: {e}"))
else:
	st.info(TT("ä¸Šä¼  2 ä¸ªåŠä»¥ä¸Šæ–‡ä»¶åï¼Œå°†åœ¨æ­¤æ˜¾ç¤ºå®ƒä»¬çš„å…¬å…±åˆ—ã€‚", "Upload 2 or more files to see common columns here."))

# æ— è®ºæ˜¯å¦ä¸Šä¼ æˆåŠŸï¼Œåªè¦æœ‰å·²åŠ è½½çš„æ•°æ®é›†å°±æä¾›é€‰æ‹©å™¨
if loaded_dfs:
	with select_container:
		file_names = list(loaded_dfs.keys())
		preferred = st.session_state.get("preferred_file_name")
		if preferred in file_names:
			default_idx = file_names.index(preferred)
		else:
			default_idx = 0 if file_names else 0
		pick_name = st.selectbox(TT("é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶è¿›è¡Œé¢„è§ˆä¸åˆ†æ", "Choose a file for preview and analysis"), file_names, index=default_idx, key="file_picker_top")
		active_df = loaded_dfs.get(pick_name)
		df_source_name = pick_name

if active_df is not None:
	df = active_df  # ä¿æŒåç»­ä»£ç å˜é‡åä¸å˜

	# å°†â€œå½“å‰æ´»åŠ¨æ•°æ®é›† + æ•°æ®é¢„è§ˆ + æ•°æ®æ¦‚è§ˆâ€ä¸Šç§»åˆ°åˆå¹¶å·¥å…·ä¸Šæ–¹çš„å®¹å™¨ä¸­
	with preview_container:
		st.info(TT(f"å½“å‰æ´»åŠ¨æ•°æ®é›†: {df_source_name}; å½¢çŠ¶: {df.shape}", f"Active dataset: {df_source_name}; shape: {df.shape}"))
		st.write(TT("æ•°æ®é¢„è§ˆï¼š", "Data preview:"))
		st.dataframe(df.head())

		#ï¼ˆå·²ç§»é™¤å¤šæ–‡ä»¶ä¸»åˆ—åŒ¹é…åŠŸèƒ½ï¼‰
		with st.expander(TT("ğŸ” æ•°æ®æ¦‚è§ˆ", "ğŸ” Data Overview"), expanded=False):
			st.write(TT("æ•°æ®æè¿°ï¼š", "Describe:"))
			st.write(df.describe(include='all').transpose())
			st.write(TT("ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š", "Missing values:"))
			st.write(df.isnull().sum())

	# ----------- åˆ¤å®šæŒ‰é’®ä¸ç»“æœå±•ç¤ºåŒºå— -------------
	# ä¼˜å…ˆä½¿ç”¨æœ€è¿‘ä¸€æ¬¡åˆå¹¶ç»“æœä½œä¸º LLM åˆ†ææ•°æ®æº
	analysis_df = st.session_state.get("train_df", df)
	analysis_df_name = st.session_state.get("train_source_name", df_source_name)
	st.caption(TT(f"AI åˆ†ææ•°æ®æºï¼š{analysis_df_name}", f"AI analysis source: {analysis_df_name}"))

	# æ„å»ºç®€æ˜“ profileï¼›åç»­å¯æ›¿æ¢ä¸º ingest.profile
	prof = {
		"columns": [
			{"name": c, "dtype": str(analysis_df[c].dtype), "missing": int(analysis_df[c].isnull().sum()), "unique": int(analysis_df[c].nunique())}
			for c in analysis_df.columns
		]
	}															

	user_question = st.text_area(TT("ä½ çš„é—®é¢˜ï¼ˆå¯é€‰ï¼‰", "Your question (optional)"), placeholder=TT("ä¾‹å¦‚ï¼šæˆ‘ä»¬èƒ½å¦é¢„æµ‹ä¹˜å®¢æ˜¯å¦ç”Ÿè¿˜ï¼Ÿæˆ– é¢„æµ‹ä»·æ ¼/åˆ†ç¾¤ç­‰ã€‚", "e.g., Can we predict survival? price? clustering?"))

	col1, col2, col3 = st.columns([1,1,1])
	with col1:
		if st.button(TT("ğŸ” å‘ç°ç ”ç©¶é—®é¢˜", "ğŸ” Discover research questions")):
			with st.spinner(TT("AI æ­£åœ¨åˆ†ææ•°æ®ï¼Œå¯»æ‰¾æœ‰ä»·å€¼çš„ç ”ç©¶é—®é¢˜ä¸æ¸…æ´—å»ºè®®...", "AI is analyzing data to suggest research questions and cleaning tips...")):
				from llm_agent import suggest_research_questions, suggest_cleaning_suggestions
				research_suggestions = suggest_research_questions(prof)
				# åŒæ­¥ç”Ÿæˆ æ¸…æ´—å»ºè®®
				clean_suggest = suggest_cleaning_suggestions(prof, user_question or "")
			st.session_state["research_suggestions"] = research_suggestions
			st.session_state["cleaning_suggest"] = clean_suggest
			st.success(TT("é—®é¢˜å‘ç°å®Œæˆï¼", "Discovery completed!"))
	
	with col2:
		if st.button(TT("ğŸ¤– æ™ºèƒ½åˆ¤å®šä»»åŠ¡", "ğŸ¤– Smart task detection")):
			with st.spinner(TT("è°ƒç”¨ OpenAI åˆ¤å®šä»»åŠ¡ç±»å‹ï¼Œå¹¶ç”Ÿæˆæ¸…æ´—å»ºè®®...", "Calling OpenAI to detect task type and generate cleaning suggestions...")):
				from llm_agent import suggest_cleaning_suggestions
				plan = detect_task(prof, user_question or "")
				# åŒæ­¥ç”Ÿæˆ æ¸…æ´—å»ºè®®
				clean_suggest = suggest_cleaning_suggestions(prof, user_question or "")
			st.session_state["plan"] = plan
			st.session_state["cleaning_suggest"] = clean_suggest
			st.success(TT("ä»»åŠ¡åˆ¤å®šå®Œæˆï¼", "Task detection completed!"))
	
	with col3:
		if "plan" in st.session_state:
			st.download_button(TT("ğŸ“„ ä¸‹è½½åˆ¤å®šç»“æœ", "ğŸ“„ Download detection result"), data=json.dumps(st.session_state["plan"], ensure_ascii=False, indent=2),
							   file_name="task_plan.json", mime="application/json")

	# æ–°å¢ï¼šç›®æ ‡åˆ—ä¸ç‰¹å¾ä¿ç•™/åˆ é™¤å»ºè®®
	col_a, col_b = st.columns([1,2])
	with col_a:
		if st.button(TT("ğŸ¯ ç›®æ ‡ä¸ç‰¹å¾å»ºè®®", "ğŸ¯ Target & feature suggestions")):
			with st.spinner(TT("AI æ­£åœ¨åˆ†æç›®æ ‡åˆ—ä¸åº”ä¿ç•™/åˆ é™¤çš„åˆ—...", "AI is analyzing target column and keep/drop features...")):
				from llm_agent import suggest_target_and_features
				feat_suggest = suggest_target_and_features(prof, user_question or "")
			st.session_state["feature_suggest"] = feat_suggest
			st.success(TT("åˆ—å»ºè®®å·²ç”Ÿæˆï¼", "Feature suggestions generated!"))
	with col_b:
		if st.session_state.get("feature_suggest"):
			st.caption(TT("ä½ å¯ä»¥å°†å»ºè®®ç›´æ¥åº”ç”¨åˆ°åç»­è®­ç»ƒçš„æ•°æ®åˆ—ä¸­ã€‚", "You can directly apply suggestions to the training columns."))

	# æ˜¾ç¤ºç ”ç©¶é—®é¢˜å»ºè®®
	if "research_suggestions" in st.session_state:
		st.subheader(TT("ğŸ” AI æ•°æ®æ´å¯Ÿï¼šå¯ç ”ç©¶çš„é—®é¢˜", "ğŸ” AI Data Insights: Researchable Questions"))
		
		suggestions = st.session_state["research_suggestions"]
		
		def display_research_suggestions(suggestions):
			"""å¯è¯»åŒ–æ˜¾ç¤ºç ”ç©¶é—®é¢˜å»ºè®®"""
			
			# æ£€æŸ¥ suggestions æ˜¯å¦ä¸ºå­—å…¸
			if not isinstance(suggestions, dict):
				st.error(TT("âŒ ç ”ç©¶å»ºè®®æ•°æ®æ ¼å¼é”™è¯¯", "âŒ Invalid format for research suggestions"))
				st.json(suggestions)
				return
			
			# ç ”ç©¶é—®é¢˜
			questions = suggestions.get("research_questions", [])
			if questions:
				st.markdown(TT("### ğŸ’¡ **æ¨èç ”ç©¶é—®é¢˜**", "### ğŸ’¡ Recommended Research Questions"))
				
				for i, q in enumerate(questions):
					with st.expander(TT(f"ğŸ“‹ é—®é¢˜ {i+1}: {q.get('question', 'æœªçŸ¥é—®é¢˜')}", f"ğŸ“‹ Question {i+1}: {q.get('question', 'Unknown')}"), expanded=i==0):
						col1, col2 = st.columns(2)
						with col1:
							st.markdown(TT(f"**ç±»å‹**: {q.get('type', 'æœªçŸ¥')}", f"**Type**: {q.get('type', 'Unknown')}"))
							st.markdown(TT(f"**éš¾åº¦**: {q.get('difficulty', 'æœªçŸ¥')}", f"**Difficulty**: {q.get('difficulty', 'Unknown')}"))
						with col2:
							if q.get('target_column'):
								st.markdown(TT(f"**ç›®æ ‡åˆ—**: `{q.get('target_column')}`", f"**Target**: `{q.get('target_column')}`"))
							methods = q.get('required_methods', [])
							if methods:
								st.markdown(TT(f"**æ¨èæ–¹æ³•**: {', '.join(methods)}", f"**Recommended methods**: {', '.join(methods)}"))
						
						st.markdown(TT("**å•†ä¸šä»·å€¼**:", "**Business value**:"))
						st.info(q.get('business_value', TT('æœªæä¾›', 'Not provided')))
			
			# åº”ç”¨åœºæ™¯
			scenarios = suggestions.get("application_scenarios", [])
			if scenarios:
				st.markdown(TT("### ğŸ¯ **åº”ç”¨åœºæ™¯**", "### ğŸ¯ Application Scenarios"))
				for i, scenario in enumerate(scenarios):
					st.markdown(f"**{i+1}.** {scenario}")
			
			# å…³é”®æ´å¯Ÿæ½œåŠ›
			insights = suggestions.get("key_insights_potential", [])
			if insights:
				st.markdown(TT("### ğŸ”® **å¯èƒ½å‘ç°çš„æ´å¯Ÿ**", "### ğŸ”® Potential Insights"))
				for insight in insights:
					st.markdown(f"â€¢ {insight}")
			
			# æ•°æ®é›†ä¼˜åŠ¿
			strengths = suggestions.get("dataset_strengths", [])
			if strengths:
				st.markdown(TT("### âœ¨ **æ•°æ®é›†ä¼˜åŠ¿**", "### âœ¨ Dataset Strengths"))
				for strength in strengths:
					st.markdown(f"âœ… {strength}")
			
			# é™åˆ¶å’Œæ³¨æ„äº‹é¡¹
			limitations = suggestions.get("limitations", [])
			if limitations:
				st.markdown(TT("### âš ï¸ **æ³¨æ„äº‹é¡¹**", "### âš ï¸ Caveats"))
				for limitation in limitations:
					st.warning(f"âš ï¸ {limitation}")

			# å»ºè®®
			recommendations = suggestions.get("recommendations", {})
			if recommendations:
				st.markdown(TT("### ğŸ¯ **è¡ŒåŠ¨å»ºè®®**", "### ğŸ¯ Recommendations"))
				# æ£€æŸ¥ recommendations æ˜¯å¦ä¸ºå­—å…¸
				if isinstance(recommendations, dict):
					priority = recommendations.get("priority_questions", [])
					if priority:
						st.markdown(TT("**ğŸ”¥ ä¼˜å…ˆç ”ç©¶é—®é¢˜**:", "**ğŸ”¥ Priority questions**:"))
						for p in priority:
							st.markdown(f"â€¢ {p}")
					
					next_steps = recommendations.get("next_steps", [])
					if next_steps:
						st.markdown(TT("**ğŸ“‹ å»ºè®®æ­¥éª¤**:", "**ğŸ“‹ Suggested steps**:"))
						for step in next_steps:
							st.markdown(f"â€¢ {step}")
					
					additional_data = recommendations.get("additional_data", [])
					if additional_data:
						st.markdown(TT("**ğŸ“Š å¯èƒ½éœ€è¦çš„é¢å¤–æ•°æ®**:", "**ğŸ“Š Additional data needed**:"))
						for data in additional_data:
							st.markdown(f"â€¢ {data}")
				else:
					# å¦‚æœ recommendations æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥æ˜¾ç¤º
					st.markdown(TT(f"**ğŸ’¡ å»ºè®®**: {recommendations}", f"**ğŸ’¡ Suggestion**: {recommendations}"))
		
		display_research_suggestions(suggestions)

		# ç ”ç©¶é—®é¢˜åˆ†æç»“è®ºæŒ‰é’®ä¸å±•ç¤º
		st.markdown(TT("### ğŸ§  ç ”ç©¶é—®é¢˜åˆ†æç»“è®º", "### ğŸ§  Research Question Analysis"))
		col_rqa1, col_rqa2 = st.columns([1,2])
		with col_rqa1:
			if st.button(TT("åˆ†æç ”ç©¶é—®é¢˜ç»“è®º", "Analyze research conclusions"), key="btn_analyze_research"):
				from llm_agent import analyze_research_questions
				with st.spinner(TT("AI æ­£åœ¨ç»¼åˆç ”ç©¶é—®é¢˜å¹¶ç”Ÿæˆç»“è®º...", "AI synthesizing research questions...")):
					res_analysis = analyze_research_questions(suggestions, st.session_state.get("profile_for_report"))
				st.session_state["__research_analysis__"] = res_analysis
				st.success(TT("ç ”ç©¶é—®é¢˜åˆ†æå®Œæˆ", "Research analysis complete"))
		with col_rqa2:
			if st.session_state.get("__research_analysis__"):
				ra = st.session_state["__research_analysis__"]
				st.markdown(ra.get("markdown","(no analysis)"))
				st.download_button(
					TT("â¬‡ï¸ ä¸‹è½½ç ”ç©¶åˆ†æ Markdown", "â¬‡ï¸ Download research analysis"),
					data=ra.get("markdown",""),
					file_name="research_analysis.md",
					mime="text/markdown"
				)
		
		# åŸå§‹ JSON (å¯é€‰)
		with st.expander(TT("ğŸ” æŸ¥çœ‹è¯¦ç»†åˆ†æç»“æœ (JSON)", "ğŸ” View detailed analysis (JSON)"), expanded=False):
			st.json(suggestions)
		
		st.success(TT("ğŸ’¡ åŸºäºä»¥ä¸Šåˆ†æï¼Œä½ å¯ä»¥é€‰æ‹©æ„Ÿå…´è¶£çš„é—®é¢˜è¿›è¡Œæ·±å…¥ç ”ç©¶ï¼", "ğŸ’¡ Based on the above, pick questions to dive deeper!"))

	if "plan" in st.session_state:
		st.subheader(TT("ğŸ¤– AI æ™ºèƒ½åˆ¤å®šç»“æœ", "ğŸ¤– AI Task Detection Result"))
		
		plan = st.session_state["plan"]
		
		# å¯è¯»åŒ–æ˜¾ç¤ºåˆ¤å®šç»“æœ
		def display_readable_plan(plan):
			# ä»»åŠ¡ç±»å‹
			task_type = plan.get("task_type", TT("æœªçŸ¥", "unknown"))
			task_type_title_cn = {"classification": "åˆ†ç±»ä»»åŠ¡", "regression": "å›å½’ä»»åŠ¡", "clustering": "èšç±»ä»»åŠ¡"}.get(task_type, task_type)
			st.markdown(TT(f"### ğŸ“Š **ä»»åŠ¡ç±»å‹**: {task_type_title_cn}", f"### ğŸ“Š **Task type**: {task_type}"))
			if task_type == "classification":
				st.info(TT("ğŸ¯ è¿™æ˜¯ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯é¢„æµ‹ç¦»æ•£çš„ç±»åˆ«æ ‡ç­¾", "ğŸ¯ Classification: predict discrete class labels"))
			elif task_type == "regression":
				st.info(TT("ğŸ“ˆ è¿™æ˜¯ä¸€ä¸ªå›å½’ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯é¢„æµ‹è¿ç»­çš„æ•°å€¼", "ğŸ“ˆ Regression: predict continuous values"))
			elif task_type == "clustering":
				st.info(TT("ğŸ” è¿™æ˜¯ä¸€ä¸ªèšç±»ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯å‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼", "ğŸ” Clustering: discover hidden patterns"))
			
			# ç›®æ ‡å€™é€‰åˆ—
			targets = plan.get("target_candidates", [])
			if targets:
				st.markdown(TT("### ğŸ¯ **æ¨èç›®æ ‡åˆ—**", "### ğŸ¯ Suggested target columns"))
				for i, target in enumerate(targets):
					st.markdown(f"**{i+1}.** `{target}`")
			else:
				st.warning(TT("âš ï¸ æœªæ‰¾åˆ°æ˜ç¡®çš„ç›®æ ‡åˆ—ï¼Œè¯·æ‰‹åŠ¨é€‰æ‹©", "âš ï¸ No clear target column found; please select manually"))
			
			# æ¨èç®—æ³•
			algorithms = plan.get("algorithms", [])
			if algorithms:
				st.markdown(TT("### ğŸ¤– **æ¨èç®—æ³•**", "### ğŸ¤– Recommended algorithms"))
				algo_names = {
					"xgboost": TT("XGBoost (æç«¯æ¢¯åº¦æå‡)", "XGBoost (Extreme Gradient Boosting)"),
					"ridge": TT("Ridge å›å½’ (å²­å›å½’)", "Ridge Regression"),
					"knn": TT("K-è¿‘é‚»ç®—æ³•", "K-Nearest Neighbors"),
					"random_forest": TT("éšæœºæ£®æ—", "Random Forest"),
					"linear_regression": TT("çº¿æ€§å›å½’", "Linear Regression"),
					"logistic_regression": TT("é€»è¾‘å›å½’", "Logistic Regression"),
					"svm": TT("æ”¯æŒå‘é‡æœº", "Support Vector Machine"),
					"mlp": TT("å¤šå±‚æ„ŸçŸ¥æœº", "MLP (Multilayer Perceptron)")
				}
				
				cols = st.columns(min(len(algorithms), 3))
				for i, algo in enumerate(algorithms):
					with cols[i % 3]:
						algo_display = algo_names.get(algo, algo.replace('_', ' ').title())
						st.markdown(f"**{i+1}.** {algo_display}")
			
			# è¯„ä¼°æŒ‡æ ‡
			metrics = plan.get("metrics", [])
			if metrics:
				st.markdown(TT("### ğŸ“ **è¯„ä¼°æŒ‡æ ‡**", "### ğŸ“ Metrics"))
				metric_names = {
					"rmse": TT("RMSE (å‡æ–¹æ ¹è¯¯å·®)", "RMSE (root mean squared error)"),
					"mae": TT("MAE (å¹³å‡ç»å¯¹è¯¯å·®)", "MAE (mean absolute error)"), 
					"r2": TT("RÂ² (å†³å®šç³»æ•°)", "RÂ² (coefficient of determination)"),
					"accuracy": TT("å‡†ç¡®ç‡", "Accuracy"),
					"f1": TT("F1 åˆ†æ•°", "F1 score"),
					"precision": TT("ç²¾ç¡®ç‡", "Precision"),
					"recall": TT("å¬å›ç‡", "Recall"),
					"auc": TT("AUC (æ›²çº¿ä¸‹é¢ç§¯)", "AUC (area under curve)")
				}
				
				metric_cols = st.columns(min(len(metrics), 3))
				for i, metric in enumerate(metrics):
					with metric_cols[i % 3]:
						metric_display = metric_names.get(metric, metric.upper())
						st.markdown(f"**â€¢** {metric_display}")
			
			# äº¤å‰éªŒè¯è®¾ç½®
			cv_info = plan.get("cv", {})
			if cv_info:
				st.markdown(TT("### âœ… **äº¤å‰éªŒè¯è®¾ç½®**", "### âœ… Cross-validation settings"))
				folds = cv_info.get("folds", 5)
				stratified = cv_info.get("stratified", False)
				
				col1, col2 = st.columns(2)
				with col1:
					st.metric(TT("äº¤å‰éªŒè¯æŠ˜æ•°", "CV folds"), f"{folds}")
				with col2:
					stratify_text = TT("æ˜¯", "Yes") if stratified else TT("å¦", "No")
					st.metric(TT("åˆ†å±‚é‡‡æ ·", "Stratified"), stratify_text)
			
			# ç±»åˆ«ä¸å¹³è¡¡ä¿¡æ¯
			imbalance = plan.get("imbalance", {})
			if imbalance and imbalance.get("is_imbalanced"):
				st.markdown(TT("### âš–ï¸ **æ•°æ®ä¸å¹³è¡¡è­¦å‘Š**", "### âš–ï¸ Imbalance warning"))
				ratio = imbalance.get("ratio")
				if ratio:
					st.warning(TT(f"æ£€æµ‹åˆ°æ•°æ®ä¸å¹³è¡¡ï¼Œä¸»è¦ç±»åˆ«å æ¯”: {ratio:.1%}", f"Detected imbalance; majority class ratio: {ratio:.1%}"))
					st.caption(TT("å»ºè®®è€ƒè™‘ä½¿ç”¨ç±»åˆ«æƒé‡å¹³è¡¡æˆ–é‡‡æ ·æŠ€æœ¯", "Consider class weights or sampling techniques"))
		
		# æ˜¾ç¤ºå¯è¯»åŒ–ç»“æœ
		display_readable_plan(plan)
		
		# å¯é€‰æ˜¾ç¤ºåŸå§‹ JSON
		with st.expander(TT("ğŸ” æŸ¥çœ‹è¯¦ç»† JSON ç»“æœ", "ğŸ” View detailed JSON"), expanded=False):
			st.json(plan)
		
		st.caption(TT("ğŸ’¡ ä»¥ä¸Šç»“æœå°†è‡ªåŠ¨åº”ç”¨åˆ°è®­ç»ƒè®¾ç½®ä¸­ï¼Œä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨è°ƒæ•´å‚æ•°ã€‚", "ğŸ’¡ These results will auto-apply to training; you can adjust manually."))

	# å·²ç§»é™¤â€œç›®æ ‡åˆ—ä¸ç‰¹å¾é€‰æ‹©å»ºè®®â€åŠŸèƒ½ä¸åº”ç”¨å…¥å£

	# æ˜¾ç¤º æ•°æ®æ¸…æ´—å»ºè®®ï¼ˆåˆå¹¶åˆ°ä»¥ä¸Šä¸¤ä¸ªæµç¨‹åå±•ç¤ºï¼‰
	if st.session_state.get("cleaning_suggest"):
		st.subheader(TT("ğŸ§¹ æ•°æ®æ¸…æ´—å»ºè®®", "ğŸ§¹ Cleaning Suggestions"))
		cs = st.session_state["cleaning_suggest"]
		# Drop
		drops = cs.get("drop_columns", [])
		with st.expander(TT(f"ğŸ—‘ï¸ å»ºè®®åˆ é™¤åˆ—ï¼ˆ{len(drops)}ï¼‰", f"ğŸ—‘ï¸ Suggested drop columns ({len(drops)})"), expanded=False):
			if drops:
				for d in drops:
					st.write(f"- {d.get('name')}: {d.get('reason','')}")
			else:
				st.write(TT("æ— ", "None"))
		# Imputations
		imps = cs.get("imputations", [])
		with st.expander(TT(f"ğŸ§© ç¼ºå¤±å€¼å¡«å……å»ºè®®ï¼ˆ{len(imps)}ï¼‰", f"ğŸ§© Imputation suggestions ({len(imps)})"), expanded=False):
			if imps:
				for d in imps:
					st.write(f"- {d.get('name')}: {d.get('strategy')}")
			else:
				st.write(TT("æ— ", "None"))
		# Type casts and parse dates
		casts = cs.get("type_casts", [])
		pdates = cs.get("parse_dates", [])
		with st.expander(TT(f"ğŸ§­ ç±»å‹è½¬æ¢å»ºè®®ï¼ˆ{len(casts)}ï¼‰/ æ—¥æœŸè§£æï¼ˆ{len(pdates)}ï¼‰", f"ğŸ§­ Type casts ({len(casts)}) / Parse dates ({len(pdates)})"), expanded=False):
			if casts:
				for d in casts:
					st.write(f"- {d.get('name')} -> {d.get('to_dtype')}: {d.get('reason','')}")
			else:
				st.write(TT("ç±»å‹è½¬æ¢ï¼šæ— ", "Type casts: None"))
			if pdates:
				st.write(TT("æ—¥æœŸè§£æï¼š", "Parse dates:"))
				st.code("\n".join(pdates))
			else:
				st.write(TT("æ—¥æœŸè§£æï¼šæ— ", "Parse dates: None"))
		# Scaling
		scaling = cs.get("scaling", {}) or {}
		with st.expander(TT("ğŸ“ ç¼©æ”¾å»ºè®®", "ğŸ“ Scaling"), expanded=False):
			st.write(TT(f"å»ºè®®ç¼©æ”¾: {'æ˜¯' if scaling.get('apply') else 'å¦'}", f"Scale recommended: {'Yes' if scaling.get('apply') else 'No'}"))
			sc_cols = scaling.get("columns", [])
			if sc_cols:
				st.code("\n".join(sc_cols))
			else:
				st.write(TT("åˆ—ï¼šæ— ", "Columns: None"))
		# Outliers
		outliers = cs.get("outliers", {}) or {}
		with st.expander(TT("ğŸ“‰ å¼‚å¸¸å€¼å¤„ç†å»ºè®®", "ğŸ“‰ Outlier handling"), expanded=False):
			st.write(TT(f"å»ºè®®å¤„ç†: {'æ˜¯' if outliers.get('apply') else 'å¦'}; æ–¹æ³•: {outliers.get('method','iqr_clip')}", f"Apply handling: {'Yes' if outliers.get('apply') else 'No'}; Method: {outliers.get('method','iqr_clip')}"))
			out_cols = outliers.get("columns", [])
			if out_cols:
				st.code("\n".join(out_cols))
			else:
				st.write(TT("åˆ—ï¼šæ— ", "Columns: None"))
		# Text processing
		txts = cs.get("text_processing", [])
		with st.expander(TT(f"ğŸ“ æ–‡æœ¬å¤„ç†å»ºè®®ï¼ˆ{len(txts)}ï¼‰", f"ğŸ“ Text processing suggestions ({len(txts)})"), expanded=False):
			if txts:
				for d in txts:
					st.write(f"- {d.get('name')}: {d.get('suggestion')}")
			else:
				st.write(TT("æ— ", "None"))
		# Leakage
		leaks = cs.get("leakage_risk", [])
		with st.expander(TT(f"âš ï¸ å¯èƒ½çš„æ³„éœ²é£é™©åˆ—ï¼ˆ{len(leaks)}ï¼‰", f"âš ï¸ Potential leakage columns ({len(leaks)})"), expanded=False):
			if leaks:
				st.code("\n".join(leaks))
			else:
				st.write(TT("æ— ", "None"))
		st.caption(cs.get("notes") or "")

		# ============ ä¸€é”®ä¸æ‰‹åŠ¨æ¸…æ´—æ“ä½œ ============
		st.markdown("---")
		st.markdown(TT("### âš™ï¸ åº”ç”¨æ¸…æ´—æ“ä½œ", "### âš™ï¸ Apply cleaning operations"))

		# å½“å‰ç”¨äºè®­ç»ƒ/åˆ†æçš„æ•°æ®
		work_df = st.session_state.get("train_df", analysis_df)
		work_name = st.session_state.get("train_source_name", analysis_df_name)
		st.caption(TT(f"æ¸…æ´—ç›®æ ‡æ•°æ®é›†ï¼š{work_name} å½¢çŠ¶ï¼š{work_df.shape}", f"Dataset to clean: {work_name} Shape: {work_df.shape}"))

		# ä¸€é”®åº”ç”¨ GPT å»ºè®®
		def _iqr_clip_inline(df, cols, whisker: float = 1.5):
			import numpy as np
			df = df.copy()
			for c in cols:
				if c in df.columns:
					try:
						s = pd.to_numeric(df[c], errors='coerce')
						q1, q3 = s.quantile(0.25), s.quantile(0.75)
						iqr = q3 - q1
						low, high = q1 - whisker * iqr, q3 + whisker * iqr
						df[c] = s.clip(lower=low, upper=high)
					except Exception:
						pass
			return df

		def _apply_type_casts(df, casts):
			df = df.copy()
			for item in casts or []:
				name = item.get('name')
				to_dtype = (item.get('to_dtype') or '').lower()
				if name not in df.columns:
					continue
				try:
					if to_dtype in ('float','float64','double','number'):
						df[name] = pd.to_numeric(df[name], errors='coerce')
					elif to_dtype in ('int','int64','long'):
						df[name] = pd.to_numeric(df[name], errors='coerce').astype('Int64')
					elif to_dtype in ('bool','boolean'):
						df[name] = df[name].astype('boolean')
					elif to_dtype in ('category','categorical'):
						df[name] = df[name].astype('category')
					elif to_dtype in ('string','str','object'):
						df[name] = df[name].astype('string')
					# else: leave as is
				except Exception:
					pass
			return df

		def _apply_imputations(df, imputations):
			df = df.copy()
			for item in imputations or []:
				name = item.get('name')
				strategy = (item.get('strategy') or 'most_frequent').lower()
				if name not in df.columns:
					continue
				try:
					if strategy == 'median':
						val = pd.to_numeric(df[name], errors='coerce').median()
						df[name] = pd.to_numeric(df[name], errors='coerce').fillna(val)
					elif strategy == 'mean':
						val = pd.to_numeric(df[name], errors='coerce').mean()
						df[name] = pd.to_numeric(df[name], errors='coerce').fillna(val)
					else:  # most_frequent
						val = df[name].mode(dropna=True)
						val = val.iloc[0] if len(val) else None
						if val is not None:
							df[name] = df[name].fillna(val)
				except Exception:
					pass
			return df

		col_btn1, col_btn2 = st.columns([1,2])
		with col_btn1:
			if st.button(TT("âš¡ ä¸€é”®åº”ç”¨ GPT æ¸…æ´—å»ºè®®", "âš¡ Apply GPT cleaning suggestions")):
				try:
					new_df = work_df.copy()
					# Drop
					to_drop = [d.get('name') for d in (cs.get('drop_columns') or []) if d.get('name') in new_df.columns]
					if to_drop:
						new_df = new_df.drop(columns=to_drop, errors='ignore')
					# Parse dates
					for c in (cs.get('parse_dates') or []):
						if c in new_df.columns:
							try:
								new_df[c] = pd.to_datetime(new_df[c], errors='coerce')
							except Exception:
								pass
					# Type casts
					new_df = _apply_type_casts(new_df, cs.get('type_casts'))
					# Imputations
					new_df = _apply_imputations(new_df, cs.get('imputations'))
					# Outliers
					out_cols = []
					out_meta = cs.get('outliers') or {}
					if isinstance(out_meta, dict) and out_meta.get('apply'):
						out_cols = [c for c in (out_meta.get('columns') or []) if c in new_df.columns]
					if out_cols:
						new_df = _iqr_clip_inline(new_df, out_cols)

					st.session_state["train_df"] = new_df
					st.session_state["train_source_name"] = TT(f"{work_name}ï¼ˆå·²æŒ‰GPTå»ºè®®æ¸…æ´—ï¼‰", f"{work_name} (cleaned by GPT suggestions)")
					st.success(TT(f"å·²åº”ç”¨ GPT æ¸…æ´—å»ºè®®ï¼Œå½“å‰å½¢çŠ¶ï¼š{new_df.shape}", f"Applied GPT cleaning; current shape: {new_df.shape}"))
				except Exception as e:
					st.error(TT(f"åº”ç”¨å¤±è´¥ï¼š{e}", f"Apply failed: {e}"))

		with col_btn2:
			st.caption(TT("æˆ–æ‰‹åŠ¨é€‰æ‹©ä»¥ä¸‹æ¸…æ´—æ“ä½œï¼š", "Or manually select operations below:"))
			# æ‰‹åŠ¨é€‰æ‹©
			all_cols = list(work_df.columns)
			default_drop = [d.get('name') for d in (cs.get('drop_columns') or []) if d.get('name') in all_cols]
			pick_drop = st.multiselect(TT("è¦åˆ é™¤çš„åˆ—", "Columns to drop"), options=all_cols, default=default_drop)

			default_dates = [c for c in (cs.get('parse_dates') or []) if c in all_cols]
			pick_dates = st.multiselect(TT("è¦è§£æä¸ºæ—¥æœŸçš„åˆ—", "Columns to parse as dates"), options=all_cols, default=default_dates)

			out_meta = cs.get('outliers') or {}
			default_out = [c for c in (out_meta.get('columns') or []) if c in all_cols]
			pick_outliers = st.multiselect(TT("IQR è£å‰ªçš„æ•°å€¼åˆ—", "Numeric columns for IQR clipping"), options=all_cols, default=default_out)

			apply_imp = st.checkbox(TT("æŒ‰å»ºè®®å¡«å……ç¼ºå¤±å€¼ï¼ˆæ•°å€¼: ä¸­ä½/å‡å€¼ï¼›ç±»åˆ«: ä¼—æ•°ï¼‰", "Impute missing values as suggested (numeric: median/mean; categorical: mode)"), value=True, key="apply_imputations_suggested")
			apply_casts = st.checkbox(TT("æŒ‰å»ºè®®è¿›è¡Œç±»å‹è½¬æ¢", "Apply suggested type casts"), value=True, key="apply_type_casts_suggested")

			if st.button(TT("ğŸ› ï¸ åº”ç”¨é€‰ä¸­æ¸…æ´—æ“ä½œ", "ğŸ› ï¸ Apply selected operations")):
				try:
					new_df = work_df.copy()
					if pick_drop:
						new_df = new_df.drop(columns=pick_drop, errors='ignore')
					for c in pick_dates:
						if c in new_df.columns:
							try:
								new_df[c] = pd.to_datetime(new_df[c], errors='coerce')
							except Exception:
								pass
					if apply_casts:
						new_df = _apply_type_casts(new_df, cs.get('type_casts'))
					if apply_imp:
						new_df = _apply_imputations(new_df, cs.get('imputations'))
					if pick_outliers:
						new_df = _iqr_clip_inline(new_df, pick_outliers)
					st.session_state["train_df"] = new_df
					st.session_state["train_source_name"] = TT(f"{work_name}ï¼ˆå·²æŒ‰æ‰‹åŠ¨æ¸…æ´—ï¼‰", f"{work_name} (cleaned manually)")
					st.success(TT(f"å·²åº”ç”¨æ‰‹åŠ¨æ¸…æ´—ï¼Œå½“å‰å½¢çŠ¶ï¼š{new_df.shape}", f"Applied manual cleaning; current shape: {new_df.shape}"))
				except Exception as e:
						st.error(TT(f"åº”ç”¨å¤±è´¥ï¼š{e}", f"Apply failed: {e}"))

		# ============ æ¸…æ´—åæ•°æ®å¯è§†åŒ– ============
		st.markdown("---")
		st.markdown(TT("### ğŸ“Š æ•°æ®å¯è§†åŒ–ï¼ˆæ¸…æ´—åï¼‰", "### ğŸ“Š Visualization (after cleaning)"))
		viz_df = st.session_state.get("train_df", work_df)
		viz_name = st.session_state.get("train_source_name", work_name)
		st.caption(TT(f"åŸºäºæ¸…æ´—åçš„æ•°æ®ï¼š{viz_name} ï¼›å½¢çŠ¶ï¼š{viz_df.shape}", f"Using cleaned data: {viz_name} ; shape: {viz_df.shape}"))

		enable_viz = st.checkbox(TT("å¯ç”¨å¯è§†åŒ–", "Enable visualization"), value=True, key="enable_viz_after_clean")
		if enable_viz and viz_df is not None and len(viz_df.columns) > 0:
			try:
				import altair as alt
				_has_altair = True
			except Exception:
				alt = None
				_has_altair = False
			col_left, col_right = st.columns([1,2])
			with col_left:
				picked_col = st.selectbox(TT("é€‰æ‹©è¦å¯è§†åŒ–çš„åˆ—", "Column to visualize"), options=list(viz_df.columns), key="viz_col_select")
				if picked_col is not None:
					series = viz_df[picked_col]
					is_num = pd.api.types.is_numeric_dtype(series)
					is_dt = pd.api.types.is_datetime64_any_dtype(series)
					if is_num:
						bins = st.slider(TT("ç›´æ–¹å›¾åˆ†ç®±æ•°", "Histogram bins"), min_value=5, max_value=100, value=30, step=5, key="viz_bins")
					elif is_dt:
						freq = st.selectbox(TT("æ—¶é—´èšåˆç²’åº¦", "Time aggregation"), ["D","W","M"], index=0, help=TT("æŒ‰å¤©/å‘¨/æœˆç»Ÿè®¡è®¡æ•°", "Group by Day/Week/Month"), key="viz_dt_freq")
					else:
						topk = st.slider(TT("ç±»åˆ«Top N", "Top-N categories"), min_value=5, max_value=100, value=20, step=5, key="viz_topk")

			with col_right:
				if 'picked_col' in locals() and picked_col is not None:
					series = viz_df[picked_col]
					is_num = pd.api.types.is_numeric_dtype(series)
					is_dt = pd.api.types.is_datetime64_any_dtype(series)
					if is_num:
						# æ•°å€¼ï¼šç›´æ–¹å›¾ + ç®±çº¿å›¾ï¼ˆæ—  Altair æ—¶é€€åŒ–ä¸ºæŸ±çŠ¶å›¾ï¼‰
						base_df = pd.DataFrame({picked_col: pd.to_numeric(series, errors='coerce')})
						if _has_altair:
							hist = alt.Chart(base_df).mark_bar().encode(
								alt.X(f"{picked_col}:Q", bin=alt.Bin(maxbins=bins)),
								y='count()'
							).properties(height=220)
							box = alt.Chart(base_df).mark_boxplot().encode(x=alt.X(f"{picked_col}:Q")).properties(height=120)
							st.altair_chart(hist & box, use_container_width=True)
						else:
							# è®¡ç®—ç›´æ–¹å›¾æ•°æ®å¹¶ç”¨åŸç”Ÿ bar_chart å±•ç¤º
							try:
								import numpy as np
								counts, bin_edges = np.histogram(base_df[picked_col].dropna(), bins=bins)
								centers = (bin_edges[:-1] + bin_edges[1:]) / 2
								df_hist = pd.DataFrame({"bin": centers, "count": counts})
								st.bar_chart(df_hist.set_index("bin")["count"])
							except Exception:
								st.line_chart(base_df[picked_col])
					elif is_dt:
						# æ—¶é—´ï¼šæŒ‰ç²’åº¦è®¡æ•°
						df_dt = pd.DataFrame({picked_col: pd.to_datetime(series, errors='coerce')}).dropna()
						if not df_dt.empty:
							df_dt['__bucket__'] = df_dt[picked_col].dt.to_period(freq).dt.start_time
							cnt = df_dt.groupby('__bucket__').size().reset_index(name='count')
							if _has_altair:
								chart = alt.Chart(cnt).mark_bar().encode(x='__bucket__:T', y='count:Q').properties(height=260)
								st.altair_chart(chart, use_container_width=True)
							else:
								cnt = cnt.set_index('__bucket__')
								st.bar_chart(cnt['count'])
						else:
							st.info(TT("æ‰€é€‰åˆ—æ— æ³•è§£æä¸ºæœ‰æ•ˆæ—¶é—´æ ¼å¼ã€‚", "Selected column cannot be parsed as valid datetime."))
					else:
						# ç±»åˆ«ï¼šTopK é¢‘æ¬¡æ¡å½¢å›¾
						vc = series.astype('string').value_counts().reset_index()
						vc.columns = [picked_col, 'count']
						vc = vc.head(topk)
						if _has_altair:
							chart = alt.Chart(vc).mark_bar().encode(
								y=alt.Y(f"{picked_col}:N", sort='-x'),
								x=alt.X('count:Q')
							).properties(height=max(200, 16*len(vc)))
							st.altair_chart(chart, use_container_width=True)
						else:
							st.bar_chart(vc.set_index(picked_col)['count'])
		else:
			st.info(TT("æ— å¯è§†åŒ–æ•°æ®å¯ç”¨æˆ–æœªé€‰æ‹©åˆ—ã€‚", "No data available for visualization or no column selected."))

	# ------------------ è®­ç»ƒè®¾ç½®ä¸è®­ç»ƒæµç¨‹ï¼ˆæœ€å°æ¥å…¥ï¼‰ ------------------
	import sys, os
	sys.path.append(os.path.dirname(os.path.dirname(__file__)))
	from core import cleandata, train as train_core

	st.subheader(TT("ğŸ› ï¸ è®­ç»ƒè®¾ç½®ï¼ˆæœ¬åœ°ï¼‰", "ğŸ› ï¸ Training Settings (local)"))

	# è®­ç»ƒæ•°æ®æ¥æºé€‰æ‹©ï¼šé»˜è®¤ä½¿ç”¨æœ€æ–°åˆå¹¶ç»“æœï¼ˆè‹¥å­˜åœ¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨å½“å‰æ´»åŠ¨æ•°æ®é›†
	options = []
	if "train_df" in st.session_state and "train_source_name" in st.session_state:
		options.append(TT(f"æœ€æ–°åˆå¹¶ç»“æœï¼ˆ{st.session_state['train_source_name']}ï¼‰", f"Latest merged result ({st.session_state['train_source_name']})"))
	options.append(TT(f"å½“å‰æ´»åŠ¨æ•°æ®é›†ï¼ˆ{df_source_name}ï¼‰", f"Active dataset ({df_source_name})"))
	default_idx = 0 if options and options[0].startswith("æœ€æ–°åˆå¹¶ç»“æœ") else 0
	selected_source = st.radio(TT("è®­ç»ƒæ•°æ®æ¥æº", "Training data source"), options, index=default_idx, horizontal=True)
	if (selected_source.startswith("æœ€æ–°åˆå¹¶ç»“æœ") or selected_source.startswith("Latest merged")) and "train_df" in st.session_state:
		train_df = st.session_state["train_df"]
		train_source_name = st.session_state.get("train_source_name", TT("æœ€æ–°åˆå¹¶ç»“æœ", "Latest merged result"))
	else:
		train_df = df
		train_source_name = df_source_name
	st.info(TT(f"è®­ç»ƒæ•°æ®é›†ï¼š{train_source_name}ï¼›å½¢çŠ¶ï¼š{train_df.shape}", f"Training dataset: {train_source_name}; shape: {train_df.shape}"))
	
	# æ™ºèƒ½åº”ç”¨ AI åˆ¤å®šç»“æœ
	plan = st.session_state.get("plan", {})
	ai_targets = plan.get("target_candidates", [])
	ai_task_type = plan.get("task_type", "")
	ai_algorithms = plan.get("algorithms", [])
	ai_cv = plan.get("cv", {})
	
	# ç›®æ ‡åˆ—é€‰æ‹© - ä¼˜å…ˆä½¿ç”¨ AI æ¨è
	available_columns = [c for c in train_df.columns if c != ""]
	if ai_targets and ai_targets[0] in available_columns:
		default_target_index = available_columns.index(ai_targets[0])
		st.success(TT(f"ğŸ¤– AI æ¨èç›®æ ‡åˆ—: {ai_targets[0]}", f"ğŸ¤– AI suggested target: {ai_targets[0]}"))
	else:
		default_target_index = 0
	
	target = st.selectbox(TT("ç›®æ ‡åˆ—", "Target column"), options=available_columns, index=default_target_index)
	
	# è‡ªåŠ¨æ¨èä»»åŠ¡ç±»å‹
	if target:
		target_series = train_df[target]
		target_series = target_series.dropna()  # å»é™¤ç¼ºå¤±å€¼
		
		# æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹
		is_numeric = pd.api.types.is_numeric_dtype(target_series)
		unique_count = target_series.nunique()
		total_count = len(target_series)
		
		# æ¨èé€»è¾‘
		if is_numeric and unique_count > 20 and unique_count / total_count > 0.05:
			recommended_task = "regression"
			reason = TT(f"æ•°å€¼ç±»å‹ï¼Œ{unique_count} ä¸ªå”¯ä¸€å€¼", f"Numeric with {unique_count} unique values")
		else:
			recommended_task = "classification" 
			if not is_numeric:
				reason = TT(f"éæ•°å€¼ç±»å‹ï¼Œ{unique_count} ä¸ªç±»åˆ«", f"Non-numeric with {unique_count} categories")
			else:
				reason = TT(f"æ•°å€¼ç±»å‹ä½†åªæœ‰ {unique_count} ä¸ªå”¯ä¸€å€¼ï¼Œå¯èƒ½æ˜¯åˆ†ç±»", f"Numeric with only {unique_count} unique values; likely classification")
		
		st.info(TT(f"ğŸ¤– æ¨èä»»åŠ¡ç±»å‹: **{recommended_task}** ({reason})", f"ğŸ¤– Suggested task: **{recommended_task}** ({reason})"))
		
		# æ˜¾ç¤ºç›®æ ‡å˜é‡çš„åŸºæœ¬ç»Ÿè®¡
		col1, col2 = st.columns(2)
		with col1:
			st.metric(TT("å”¯ä¸€å€¼æ•°é‡", "Unique values"), unique_count)
		with col2:
			st.metric(TT("æ ·æœ¬æ•°é‡", "Samples"), total_count)
		
		if unique_count <= 10:
			st.write(TT("ç›®æ ‡å˜é‡çš„å€¼åˆ†å¸ƒ:", "Target value distribution:"))
			value_counts = target_series.value_counts().head(10)
			st.bar_chart(value_counts)
	
	# ä»»åŠ¡ç±»å‹é€‰æ‹© - æ™ºèƒ½åº”ç”¨ AI æ¨è
	task_options = ["classification", "regression"]
	if ai_task_type and ai_task_type in task_options:
		default_task_index = task_options.index(ai_task_type)
		st.success(TT(f"ğŸ¤– AI æ¨èä»»åŠ¡ç±»å‹: {ai_task_type}", f"ğŸ¤– AI suggested task: {ai_task_type}"))
	else:
		default_task_index = 0
	
	task_type = st.selectbox(TT("ä»»åŠ¡ç±»å‹", "Task type"), task_options, index=default_task_index)
	
	# ç®—æ³•é€‰æ‹© - æ™ºèƒ½åº”ç”¨ AI æ¨è
	available_algos = ["logreg","rf","xgb","knn","mlp"] if task_type=="classification" else ["linreg","ridge","rf","xgb","knn","mlp"]
	
	# æ˜ å°„ AI æ¨èçš„ç®—æ³•åç§°åˆ°æœ¬åœ°ç®—æ³•åç§°
	algo_mapping = {
		"xgboost": "xgb",
		"random_forest": "rf", 
		"ridge": "ridge",
		"knn": "knn",
		"logistic_regression": "logreg",
		"linear_regression": "linreg",
		"mlp": "mlp"
	}
	
	ai_algos_mapped = []
	if ai_algorithms:
		for ai_algo in ai_algorithms:
			mapped_algo = algo_mapping.get(ai_algo, ai_algo)
			if mapped_algo in available_algos:
				ai_algos_mapped.append(mapped_algo)
		
		if ai_algos_mapped:
			st.success(TT(f"ğŸ¤– AI æ¨èç®—æ³•: {', '.join(ai_algos_mapped)}", f"ğŸ¤– AI suggested algorithms: {', '.join(ai_algos_mapped)}"))
			default_algos = ai_algos_mapped
		else:
			default_algos = ["rf","xgb"]
	else:
		default_algos = ["rf","xgb"]
	
	picked = st.multiselect(
		TT("å€™é€‰ç®—æ³•", "Candidate algorithms"),
		available_algos,
		default=default_algos
	)
	budget = st.slider(TT("æ¯æ¨¡å‹æœç´¢æ¬¡æ•° (n_iter)", "Search iterations per model (n_iter)"), 10, 80, 30)
	
	# äº¤å‰éªŒè¯æŠ˜æ•° - æ™ºèƒ½åº”ç”¨ AI æ¨è
	default_folds = ai_cv.get("folds", 5) if ai_cv else 5
	if ai_cv and "folds" in ai_cv:
		st.success(TT(f"ğŸ¤– AI æ¨è CV æŠ˜æ•°: {default_folds}", f"ğŸ¤– AI suggested CV folds: {default_folds}"))
	
	folds = st.slider(TT("CV æŠ˜æ•°", "CV folds"), 3, 10, default_folds)


	# è¯„ä¼°è¡Œæ•°é™åˆ¶è®¾ç½®
	with st.expander(TT("âš™ï¸ è¯„ä¼°æ•°æ®é‡è®¾ç½®", "âš™ï¸ Evaluation data limit"), expanded=False):
		col_a, col_b = st.columns([1,2])
		with col_a:
			use_eval_limit = st.checkbox(TT("é™åˆ¶è¯„ä¼°è¡Œæ•°", "Limit evaluation rows"), value=True, key="use_eval_limit_rows", help=TT("ä»…åœ¨è¯„ä¼°æŒ‡æ ‡/é¢„æµ‹æ—¶ä½¿ç”¨æµ‹è¯•é›†å‰ N è¡Œï¼Œé€‚åˆå¿«é€Ÿè¿­ä»£ã€‚", "Use first N rows of test set only for evaluation/prediction; faster iteration."))
		with col_b:
			if use_eval_limit:
				custom_eval_rows = st.number_input(TT("è¯„ä¼°æœ€å¤§è¡Œæ•° N", "Max evaluation rows N"), min_value=50, max_value=20000, value=500, step=50, help=TT("è¶…è¿‡è¯¥è¡Œæ•°æ—¶ä»…æˆªå–å‰ N è¡Œï¼›ä¸å½±å“æ¨¡å‹è®­ç»ƒã€‚", "If larger, only take first N rows; training unaffected."))
			else:
				custom_eval_rows = None

	if st.button(TT("å¼€å§‹è®­ç»ƒ", "Start training")):
		X_train, X_test, y_train, y_test, pre, col_info = cleandata.prepare(train_df, target, task_type)
		leaderboard, artifacts = train_core.run_all(
			X_train, y_train, X_test, y_test,
			task_type=task_type,
			picked_models=picked,
			preprocessor=pre,
			n_iter=budget,
			cv_folds=folds,
			artifacts_dir="artifacts"
		)
		st.success(TT("è®­ç»ƒå®Œæˆï¼", "Training completed!"))
		st.dataframe(leaderboard)
		# è®°å½•æœ¬æ¬¡è®­ç»ƒä¸Šä¸‹æ–‡ï¼Œä¾›åç»­ä¸€è‡´æ€§æ£€æŸ¥ä½¿ç”¨
		st.session_state["__trained_target__"] = target
		st.session_state["__trained_algos__"] = picked
		# æ ¹æ®ç”¨æˆ·è®¾ç½®é™åˆ¶è¯„ä¼°é˜¶æ®µä½¿ç”¨çš„æµ‹è¯•é›†è¡Œæ•°
		if custom_eval_rows and custom_eval_rows > 0 and len(X_test) > custom_eval_rows:
			n_eval = int(min(custom_eval_rows, len(X_test)))
			if hasattr(X_test, 'head'):
				try:
					X_test_eval = X_test.head(n_eval)
				except Exception:
					X_test_eval = X_test[:n_eval]
			else:
				X_test_eval = X_test[:n_eval]
			if hasattr(y_test, 'iloc'):
				y_test_eval = y_test.iloc[:n_eval]
			else:
				y_test_eval = y_test[:n_eval]
			st.info(TT(f"è¯„ä¼°è¡Œæ•°é™åˆ¶å¯ç”¨ï¼šä½¿ç”¨æµ‹è¯•é›†å‰ {n_eval} è¡Œï¼ˆåŸå§‹ {len(X_test)} è¡Œï¼‰ã€‚", f"Eval row limit enabled: using first {n_eval} rows (original {len(X_test)})"))
		else:
			X_test_eval = X_test
			y_test_eval = y_test
			st.info(TT(f"è¯„ä¼°è¡Œæ•°é™åˆ¶æœªå¯ç”¨ï¼Œä½¿ç”¨å…¨éƒ¨æµ‹è¯•é›† {len(X_test)} è¡Œã€‚", f"Eval row limit disabled: using all {len(X_test)} test rows."))
		st.session_state["__eval_pack__"] = (task_type, X_test_eval, y_test_eval, artifacts)

		# å­˜å‚¨æ’è¡Œæ¦œç”¨äºæŠ¥å‘Šç”Ÿæˆ
		st.session_state["leaderboard_df"] = leaderboard

		# å±•ç¤ºæ’è¡Œæ¦œå…³é”®æŒ‡æ ‡ä¸æ¨¡å‹ä¸‹è½½
		st.markdown(TT("### ğŸ è®­ç»ƒç»“æœä¸æ¨¡å‹å¯¼å‡º", "### ğŸ Training Results & Model Export"))
		# å…³é”®æŒ‡æ ‡æ‘˜è¦
		try:
			if task_type == "classification":
				metric_cols = [c for c in ["acc","f1_macro","roc_auc"] if c in leaderboard.columns]
			else:
				metric_cols = [c for c in ["rmse","mae","r2"] if c in leaderboard.columns]
			if metric_cols:
				st.caption(TT(f"å±•ç¤ºå…³é”®è¯„ä¼°æŒ‡æ ‡ï¼š{', '.join(metric_cols)}", f"Showing key metrics: {', '.join(metric_cols)}"))
				st.dataframe(leaderboard[["model","cv_score(primary)",*metric_cols,"fit_s","predict_s","params"]])
		except Exception:
			st.dataframe(leaderboard)

		# æä¾›å„æ¨¡å‹æ–‡ä»¶ä¸‹è½½
		with st.expander(TT("â¬‡ï¸ ä¸‹è½½æœ€ä½³æ¨¡å‹æ–‡ä»¶", "â¬‡ï¸ Download best model files"), expanded=True):
			for mname, info in (artifacts or {}).items():
				mpath = info.get("model_path")
				st.write(TT(f"æ¨¡å‹ï¼š{mname}", f"Model: {mname}"))
				if mpath and os.path.exists(mpath):
					try:
						with open(mpath, "rb") as fh:
							st.download_button(TT(f"ä¸‹è½½ {os.path.basename(mpath)}", f"Download {os.path.basename(mpath)}"), data=fh.read(), file_name=os.path.basename(mpath))
					except Exception as e:
						st.warning(TT(f"æ— æ³•æä¾›ä¸‹è½½ï¼š{e}", f"Cannot provide download: {e}"))
				else:
					st.info(TT("æ¨¡å‹æ–‡ä»¶å°šæœªç”Ÿæˆæˆ–è·¯å¾„ä¸å¯ç”¨ã€‚", "Model file not generated or path unavailable."))


		# ï¼ˆåˆ†ææŒ‰é’®ç§»è‡³å…¨å±€åŒºå—ï¼Œä¿è¯é‡æ–°è¿è¡Œåä»å¯ä½¿ç”¨ï¼‰

	# ------------------ æŠ¥å‘Šç”Ÿæˆï¼ˆOpenAIï¼‰ ------------------
	# å…¨å±€æ˜¾ç¤ºè®­ç»ƒç»“æœåˆ†æåŒºå—ï¼ˆè‹¥å·²æœ‰ leaderboardï¼‰ï¼Œé¿å…æŒ‰é’®åªåœ¨è®­ç»ƒå½“æ¬¡å‡ºç°
	leaderboard_existing = st.session_state.get("leaderboard_df")
	if leaderboard_existing is not None:
		st.markdown(TT("### ğŸ” è®­ç»ƒç»“æœåˆ†æ (LLM)", "### ğŸ” Training Result Analysis (LLM)"))
		# å®‰å…¨è·å– artifacts ä¸ task_type
		_eval_pack = st.session_state.get("__eval_pack__") or (None, None, None, {})
		_artifacts = _eval_pack[3] if isinstance(_eval_pack, (list, tuple)) and len(_eval_pack) == 4 else {}
		_task_type_for_analysis = (st.session_state.get("plan") or {}).get("task_type") or _eval_pack[0] or "classification"
		_plan_obj = st.session_state.get("plan")
		col_an1, col_an2 = st.columns([1,2])
		with col_an1:
			if st.button(TT("ğŸ§  åˆ†æè®­ç»ƒç»“æœ", "ğŸ§  Analyze training results"), key="btn_analyze_training_global"):
				from llm_agent import analyze_training_results
				with st.spinner(TT("AI æ­£åœ¨åˆ†æè®­ç»ƒæ’è¡Œæ¦œ...", "AI analyzing leaderboard...")):
					analysis = analyze_training_results(
						leaderboard_existing,
						_artifacts,
						_task_type_for_analysis,
						_plan_obj,
						lang=st.session_state.get("lang", "zh")
					)
				st.session_state["__training_analysis__"] = analysis
				st.success(TT("åˆ†æå®Œæˆ", "Analysis complete"))
		with col_an2:
			if st.session_state.get("__training_analysis__"):
				an = st.session_state["__training_analysis__"]
				st.markdown(an.get("markdown","(no analysis)"))
				st.download_button(
					TT("â¬‡ï¸ ä¸‹è½½è®­ç»ƒåˆ†æ Markdown", "â¬‡ï¸ Download training analysis"),
					data=an.get("markdown",""),
					file_name="training_analysis.md",
					mime="text/markdown"
				)

		# â€”â€” æ–°å¢ï¼šä¸æ¨èç ”ç©¶é—®é¢˜ä¸€è‡´æ€§æ£€æŸ¥ â€”â€”
		st.markdown(TT("### âœ… ä¸ç ”ç©¶é—®é¢˜çš„ä¸€è‡´æ€§æ£€æŸ¥", "### âœ… Alignment with Research Questions"))
		rs_suggest = st.session_state.get("research_suggestions")
		col_chk1, col_chk2 = st.columns([1,2])
		with col_chk1:
			if st.button(TT("å¯¹é½æ£€æŸ¥", "Run alignment check"), key="btn_alignment_check"):
				from llm_agent import check_research_alignment
				with st.spinner(TT("æ­£åœ¨å¯¹æ¯”è®­ç»ƒç»“æœä¸æ¨èç ”ç©¶é—®é¢˜...", "Comparing training against research questions...")):
					align = check_research_alignment(
						leaderboard_existing,
						_artifacts,
						_task_type_for_analysis,
						rs_suggest,
						trained_target=st.session_state.get("__trained_target__"),
						picked_models=st.session_state.get("__trained_algos__"),
						lang=st.session_state.get("lang","zh"),
					)
				st.session_state["__alignment_report__"] = align
				st.success(TT("å¯¹é½æ£€æŸ¥å®Œæˆ", "Alignment check complete"))
		with col_chk2:
			if st.session_state.get("__alignment_report__"):
				rep = st.session_state["__alignment_report__"]
				st.markdown(rep.get("markdown", "(no alignment result)"))
				st.download_button(
					TT("â¬‡ï¸ ä¸‹è½½å¯¹é½æŠ¥å‘Š Markdown", "â¬‡ï¸ Download alignment report"),
					data=rep.get("markdown", ""),
					file_name="alignment_with_research_questions.md",
					mime="text/markdown"
				)

		# â€”â€” æ–°å¢ï¼šåŸºäºè®­ç»ƒç»“æœâ€œå›ç­”â€ç ”ç©¶é—®é¢˜ â€”â€”
		st.markdown(TT("### ğŸ’¬ å›ç­”ç ”ç©¶é—®é¢˜", "### ğŸ’¬ Answer Research Questions"))
		col_ans1, col_ans2 = st.columns([1,2])
		with col_ans1:
			if st.button(TT("ç”Ÿæˆå›ç­”", "Generate answers"), key="btn_answer_questions"):
				from llm_agent import answer_research_questions
				with st.spinner(TT("æ­£åœ¨æ±‡æ€»æœ€ä½³æ¨¡å‹æŒ‡æ ‡å¹¶ä½œç­”...", "Summarizing best model metrics to answer...")):
					ans = answer_research_questions(
						research_suggestions=rs_suggest or {},
						profile=st.session_state.get("profile_for_report"),
						leaderboard=leaderboard_existing,
						artifacts=_artifacts,
						task_type=_task_type_for_analysis,
						trained_target=st.session_state.get("__trained_target__"),
						lang=st.session_state.get("lang","zh"),
					)
				st.session_state["__rq_answers__"] = ans
				st.success(TT("ç ”ç©¶é—®é¢˜å›ç­”å·²ç”Ÿæˆ", "Research question answers generated"))
		with col_ans2:
			if st.session_state.get("__rq_answers__"):
				ans = st.session_state["__rq_answers__"]
				st.markdown(ans.get("markdown", "(no answers)"))
				st.download_button(
					TT("â¬‡ï¸ ä¸‹è½½å›ç­” Markdown", "â¬‡ï¸ Download answers"),
					data=ans.get("markdown", ""),
					file_name="research_questions_answers.md",
					mime="text/markdown"
				)
	else:
		st.info(TT("å°šæœªè®­ç»ƒï¼Œè®­ç»ƒç»“æœåˆ†ææŒ‰é’®å°†åœ¨è®­ç»ƒå®Œæˆåå‡ºç°ã€‚", "No training yet; analysis button will appear after training."))

	st.markdown("---")
	st.subheader(TT("ğŸ“„ ç”Ÿæˆæ€»ç»“æŠ¥å‘Šï¼ˆOpenAIï¼‰", "ğŸ“„ Generate Summary Report (OpenAI)"))

	# ç»„è£…æŠ¥å‘Šä¸Šä¸‹æ–‡
	bundle = {
		"meta": {
			"dataset_name": st.session_state.get("train_source_name", df_source_name),
		},
		"profile": {
			"columns": [
				{"name": c, "dtype": str((st.session_state.get("train_df", df))[c].dtype)}
				for c in (st.session_state.get("train_df", df)).columns
			]
		},
		"research_suggestions": st.session_state.get("research_suggestions"),
		"plan": st.session_state.get("plan"),
		"cleaning_suggest": st.session_state.get("cleaning_suggest"),
		"research_suggestions": st.session_state.get("research_suggestions"),
		"leaderboard": st.session_state.get("leaderboard_df"),
		"artifacts": st.session_state.get("__eval_pack__", (None, None, None, {}))[3],
	}

	col_r1, col_r2 = st.columns([1,2])
	with col_r1:
		if st.button(TT("ğŸ§  ä½¿ç”¨ OpenAI ç”ŸæˆæŠ¥å‘Š", "ğŸ§  Generate report via OpenAI")):
			with st.spinner(TT("æ­£åœ¨ç”ŸæˆæŠ¥å‘Šâ€¦", "Generating reportâ€¦")):
				from llm_agent import write_report
				report_md = write_report(bundle, lang=st.session_state.get("lang","zh"))
				st.session_state["__final_report_md__"] = report_md
			st.success(TT("æŠ¥å‘Šå·²ç”Ÿæˆï¼", "Report generated!"))

	with col_r2:
		if st.session_state.get("__final_report_md__"):
			st.markdown(st.session_state["__final_report_md__"])
			st.download_button(
				TT("â¬‡ï¸ ä¸‹è½½æŠ¥å‘Š Markdown", "â¬‡ï¸ Download report (Markdown)"),
				data=st.session_state["__final_report_md__"],
				file_name="automl_report.md",
				mime="text/markdown"
			)
