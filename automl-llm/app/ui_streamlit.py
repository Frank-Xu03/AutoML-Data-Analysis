import streamlit as st
import os
import json
import sys
sys.path.append(os.path.dirname(__file__))
from llm_agent import detect_task
st.set_page_config(page_title="LLM-Augmented AutoML", layout="wide")
import pandas as pd

st.title("LLM-Augmented AutoML (Local Training)")
st.success("ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸã€‚æ¥ä¸‹æ¥å°†å®ç°æ•°æ®ä¸Šä¼ ã€åˆ¤å®šä¸æŠ¥å‘Šã€‚")

# æ•°æ®ä¸Šä¼ 
uploaded_file = st.file_uploader("ä¸Šä¼ æ•°æ®æ–‡ä»¶ï¼ˆCSVï¼‰", type=["csv"])

if uploaded_file is not None:
	df = pd.read_csv(uploaded_file)
	# ä¿å­˜åˆ° examples ç›®å½•
	save_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'examples', uploaded_file.name)
	# è‡ªåŠ¨åˆ›å»ºç›®å½•
	os.makedirs(os.path.dirname(save_path), exist_ok=True)
	with open(save_path, "wb") as f:
		f.write(uploaded_file.getbuffer())
	st.success(f"æ–‡ä»¶å·²ä¿å­˜åˆ° examples/{uploaded_file.name}")
	st.write("æ•°æ®é¢„è§ˆï¼š")
	st.dataframe(df.head())
	st.write("æ•°æ®æè¿°ï¼š")
	st.write(df.describe())
	st.write("ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š")
	st.write(df.isnull().sum())

	# ----------- åˆ¤å®šæŒ‰é’®ä¸ç»“æœå±•ç¤ºåŒºå— -------------
	# è¿™é‡Œå‡è®¾ prof æ˜¯æ•°æ® profileï¼Œå®é™…åº”ç”± ingest/profile ç”Ÿæˆ
	# ä½ å¯ä»¥ç”¨ df.describe().to_dict() æˆ–è‡ªå®šä¹‰ profile
	prof = {
		"columns": [
			{"name": c, "dtype": str(df[c].dtype), "missing": int(df[c].isnull().sum()), "unique": int(df[c].nunique())}
			for c in df.columns
		]
	}

	user_question = st.text_area("ä½ çš„é—®é¢˜ï¼ˆå¯é€‰ï¼‰", placeholder="ä¾‹å¦‚ï¼šæˆ‘ä»¬èƒ½å¦é¢„æµ‹ä¹˜å®¢æ˜¯å¦ç”Ÿè¿˜ï¼Ÿæˆ– é¢„æµ‹ä»·æ ¼/åˆ†ç¾¤ç­‰ã€‚")

	col1, col2, col3 = st.columns([1,1,1])
	with col1:
		if st.button("ğŸ” å‘ç°ç ”ç©¶é—®é¢˜"):
			with st.spinner("AI æ­£åœ¨åˆ†ææ•°æ®ï¼Œå¯»æ‰¾æœ‰ä»·å€¼çš„ç ”ç©¶é—®é¢˜..."):
				from llm_agent import suggest_research_questions
				research_suggestions = suggest_research_questions(prof)
			st.session_state["research_suggestions"] = research_suggestions
			st.success("é—®é¢˜å‘ç°å®Œæˆï¼")
	
	with col2:
		if st.button("ğŸ¤– æ™ºèƒ½åˆ¤å®šä»»åŠ¡"):
			with st.spinner("è°ƒç”¨ OpenAI åˆ¤å®šä»»åŠ¡ç±»å‹ä¸æ–¹æ¡ˆ..."):
				plan = detect_task(prof, user_question or "")
			st.session_state["plan"] = plan
			st.success("ä»»åŠ¡åˆ¤å®šå®Œæˆï¼")
	
	with col3:
		if "plan" in st.session_state:
			st.download_button("ğŸ“„ ä¸‹è½½åˆ¤å®šç»“æœ", data=json.dumps(st.session_state["plan"], ensure_ascii=False, indent=2),
							   file_name="task_plan.json", mime="application/json")

	# æ˜¾ç¤ºç ”ç©¶é—®é¢˜å»ºè®®
	if "research_suggestions" in st.session_state:
		st.subheader("ğŸ” AI æ•°æ®æ´å¯Ÿï¼šå¯ç ”ç©¶çš„é—®é¢˜")
		
		suggestions = st.session_state["research_suggestions"]
		
		def display_research_suggestions(suggestions):
			"""å¯è¯»åŒ–æ˜¾ç¤ºç ”ç©¶é—®é¢˜å»ºè®®"""
			
			# æ£€æŸ¥ suggestions æ˜¯å¦ä¸ºå­—å…¸
			if not isinstance(suggestions, dict):
				st.error("âŒ ç ”ç©¶å»ºè®®æ•°æ®æ ¼å¼é”™è¯¯")
				st.json(suggestions)
				return
			
			# ç ”ç©¶é—®é¢˜
			questions = suggestions.get("research_questions", [])
			if questions:
				st.markdown("### ğŸ’¡ **æ¨èç ”ç©¶é—®é¢˜**")
				
				for i, q in enumerate(questions):
					with st.expander(f"ğŸ“‹ é—®é¢˜ {i+1}: {q.get('question', 'æœªçŸ¥é—®é¢˜')}", expanded=i==0):
						col1, col2 = st.columns(2)
						with col1:
							st.markdown(f"**ç±»å‹**: {q.get('type', 'æœªçŸ¥')}")
							st.markdown(f"**éš¾åº¦**: {q.get('difficulty', 'æœªçŸ¥')}")
						with col2:
							if q.get('target_column'):
								st.markdown(f"**ç›®æ ‡åˆ—**: `{q.get('target_column')}`")
							methods = q.get('required_methods', [])
							if methods:
								st.markdown(f"**æ¨èæ–¹æ³•**: {', '.join(methods)}")
						
						st.markdown("**å•†ä¸šä»·å€¼**:")
						st.info(q.get('business_value', 'æœªæä¾›'))
			
			# åº”ç”¨åœºæ™¯
			scenarios = suggestions.get("application_scenarios", [])
			if scenarios:
				st.markdown("### ğŸ¯ **åº”ç”¨åœºæ™¯**")
				for i, scenario in enumerate(scenarios):
					st.markdown(f"**{i+1}.** {scenario}")
			
			# å…³é”®æ´å¯Ÿæ½œåŠ›
			insights = suggestions.get("key_insights_potential", [])
			if insights:
				st.markdown("### ğŸ”® **å¯èƒ½å‘ç°çš„æ´å¯Ÿ**")
				for insight in insights:
					st.markdown(f"â€¢ {insight}")
			
			# æ•°æ®é›†ä¼˜åŠ¿
			strengths = suggestions.get("dataset_strengths", [])
			if strengths:
				st.markdown("### âœ¨ **æ•°æ®é›†ä¼˜åŠ¿**")
				for strength in strengths:
					st.markdown(f"âœ… {strength}")
			
			# é™åˆ¶å’Œæ³¨æ„äº‹é¡¹
			limitations = suggestions.get("limitations", [])
			if limitations:
				st.markdown("### âš ï¸ **æ³¨æ„äº‹é¡¹**")
				for limitation in limitations:
					st.warning(f"âš ï¸ {limitation}")
			
			# å»ºè®®
			recommendations = suggestions.get("recommendations", {})
			if recommendations:
				st.markdown("### ğŸ¯ **è¡ŒåŠ¨å»ºè®®**")
				
				# æ£€æŸ¥ recommendations æ˜¯å¦ä¸ºå­—å…¸
				if isinstance(recommendations, dict):
					priority = recommendations.get("priority_questions", [])
					if priority:
						st.markdown("**ğŸ”¥ ä¼˜å…ˆç ”ç©¶é—®é¢˜**:")
						for p in priority:
							st.markdown(f"â€¢ {p}")
					
					next_steps = recommendations.get("next_steps", [])
					if next_steps:
						st.markdown("**ğŸ“‹ å»ºè®®æ­¥éª¤**:")
						for step in next_steps:
							st.markdown(f"â€¢ {step}")
					
					additional_data = recommendations.get("additional_data", [])
					if additional_data:
						st.markdown("**ğŸ“Š å¯èƒ½éœ€è¦çš„é¢å¤–æ•°æ®**:")
						for data in additional_data:
							st.markdown(f"â€¢ {data}")
				else:
					# å¦‚æœ recommendations æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥æ˜¾ç¤º
					st.markdown(f"**ğŸ’¡ å»ºè®®**: {recommendations}")
		
		display_research_suggestions(suggestions)
		
		# åŸå§‹ JSON (å¯é€‰)
		with st.expander("ğŸ” æŸ¥çœ‹è¯¦ç»†åˆ†æç»“æœ (JSON)", expanded=False):
			st.json(suggestions)
		
		st.success("ğŸ’¡ åŸºäºä»¥ä¸Šåˆ†æï¼Œä½ å¯ä»¥é€‰æ‹©æ„Ÿå…´è¶£çš„é—®é¢˜è¿›è¡Œæ·±å…¥ç ”ç©¶ï¼")

	if "plan" in st.session_state:
		st.subheader("ğŸ¤– AI æ™ºèƒ½åˆ¤å®šç»“æœ")
		
		plan = st.session_state["plan"]
		
		# å¯è¯»åŒ–æ˜¾ç¤ºåˆ¤å®šç»“æœ
		def display_readable_plan(plan):
			# ä»»åŠ¡ç±»å‹
			task_type = plan.get("task_type", "æœªçŸ¥")
			task_type_cn = {"classification": "åˆ†ç±»ä»»åŠ¡", "regression": "å›å½’ä»»åŠ¡", "clustering": "èšç±»ä»»åŠ¡"}.get(task_type, task_type)
			
			st.markdown(f"### ğŸ“Š **ä»»åŠ¡ç±»å‹**: {task_type_cn}")
			if task_type == "classification":
				st.info("ğŸ¯ è¿™æ˜¯ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯é¢„æµ‹ç¦»æ•£çš„ç±»åˆ«æ ‡ç­¾")
			elif task_type == "regression":
				st.info("ğŸ“ˆ è¿™æ˜¯ä¸€ä¸ªå›å½’ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯é¢„æµ‹è¿ç»­çš„æ•°å€¼")
			elif task_type == "clustering":
				st.info("ğŸ” è¿™æ˜¯ä¸€ä¸ªèšç±»ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯å‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼")
			
			# ç›®æ ‡å€™é€‰åˆ—
			targets = plan.get("target_candidates", [])
			if targets:
				st.markdown("### ğŸ¯ **æ¨èç›®æ ‡åˆ—**")
				for i, target in enumerate(targets):
					st.markdown(f"**{i+1}.** `{target}`")
			else:
				st.warning("âš ï¸ æœªæ‰¾åˆ°æ˜ç¡®çš„ç›®æ ‡åˆ—ï¼Œè¯·æ‰‹åŠ¨é€‰æ‹©")
			
			# æ¨èç®—æ³•
			algorithms = plan.get("algorithms", [])
			if algorithms:
				st.markdown("### ğŸ¤– **æ¨èç®—æ³•**")
				algo_names = {
					"xgboost": "XGBoost (æç«¯æ¢¯åº¦æå‡)",
					"ridge": "Ridge å›å½’ (å²­å›å½’)",
					"knn": "K-è¿‘é‚»ç®—æ³•",
					"random_forest": "éšæœºæ£®æ—",
					"linear_regression": "çº¿æ€§å›å½’",
					"logistic_regression": "é€»è¾‘å›å½’",
					"svm": "æ”¯æŒå‘é‡æœº",
					"mlp": "å¤šå±‚æ„ŸçŸ¥æœº"
				}
				
				cols = st.columns(min(len(algorithms), 3))
				for i, algo in enumerate(algorithms):
					with cols[i % 3]:
						algo_display = algo_names.get(algo, algo.replace('_', ' ').title())
						st.markdown(f"**{i+1}.** {algo_display}")
			
			# è¯„ä¼°æŒ‡æ ‡
			metrics = plan.get("metrics", [])
			if metrics:
				st.markdown("### ğŸ“ **è¯„ä¼°æŒ‡æ ‡**")
				metric_names = {
					"rmse": "RMSE (å‡æ–¹æ ¹è¯¯å·®)",
					"mae": "MAE (å¹³å‡ç»å¯¹è¯¯å·®)", 
					"r2": "RÂ² (å†³å®šç³»æ•°)",
					"accuracy": "å‡†ç¡®ç‡",
					"f1": "F1 åˆ†æ•°",
					"precision": "ç²¾ç¡®ç‡",
					"recall": "å¬å›ç‡",
					"auc": "AUC (æ›²çº¿ä¸‹é¢ç§¯)"
				}
				
				metric_cols = st.columns(min(len(metrics), 3))
				for i, metric in enumerate(metrics):
					with metric_cols[i % 3]:
						metric_display = metric_names.get(metric, metric.upper())
						st.markdown(f"**â€¢** {metric_display}")
			
			# äº¤å‰éªŒè¯è®¾ç½®
			cv_info = plan.get("cv", {})
			if cv_info:
				st.markdown("### âœ… **äº¤å‰éªŒè¯è®¾ç½®**")
				folds = cv_info.get("folds", 5)
				stratified = cv_info.get("stratified", False)
				
				col1, col2 = st.columns(2)
				with col1:
					st.metric("äº¤å‰éªŒè¯æŠ˜æ•°", f"{folds} æŠ˜")
				with col2:
					stratify_text = "æ˜¯" if stratified else "å¦"
					st.metric("åˆ†å±‚é‡‡æ ·", stratify_text)
			
			# ç±»åˆ«ä¸å¹³è¡¡ä¿¡æ¯
			imbalance = plan.get("imbalance", {})
			if imbalance and imbalance.get("is_imbalanced"):
				st.markdown("### âš–ï¸ **æ•°æ®ä¸å¹³è¡¡è­¦å‘Š**")
				ratio = imbalance.get("ratio")
				if ratio:
					st.warning(f"æ£€æµ‹åˆ°æ•°æ®ä¸å¹³è¡¡ï¼Œä¸»è¦ç±»åˆ«å æ¯”: {ratio:.1%}")
					st.caption("å»ºè®®è€ƒè™‘ä½¿ç”¨ç±»åˆ«æƒé‡å¹³è¡¡æˆ–é‡‡æ ·æŠ€æœ¯")
		
		# æ˜¾ç¤ºå¯è¯»åŒ–ç»“æœ
		display_readable_plan(plan)
		
		# å¯é€‰æ˜¾ç¤ºåŸå§‹ JSON
		with st.expander("ğŸ” æŸ¥çœ‹è¯¦ç»† JSON ç»“æœ", expanded=False):
			st.json(plan)
		
		st.caption("ğŸ’¡ ä»¥ä¸Šç»“æœå°†è‡ªåŠ¨åº”ç”¨åˆ°è®­ç»ƒè®¾ç½®ä¸­ï¼Œä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨è°ƒæ•´å‚æ•°ã€‚")

	# ------------------ è®­ç»ƒè®¾ç½®ä¸è®­ç»ƒæµç¨‹ï¼ˆæœ€å°æ¥å…¥ï¼‰ ------------------
	import sys, os
	sys.path.append(os.path.dirname(os.path.dirname(__file__)))
	from core import cleandata, train as train_core

	st.subheader("ğŸ› ï¸ è®­ç»ƒè®¾ç½®ï¼ˆæœ¬åœ°ï¼‰")
	
	# æ™ºèƒ½åº”ç”¨ AI åˆ¤å®šç»“æœ
	plan = st.session_state.get("plan", {})
	ai_targets = plan.get("target_candidates", [])
	ai_task_type = plan.get("task_type", "")
	ai_algorithms = plan.get("algorithms", [])
	ai_cv = plan.get("cv", {})
	
	# ç›®æ ‡åˆ—é€‰æ‹© - ä¼˜å…ˆä½¿ç”¨ AI æ¨è
	available_columns = [c for c in df.columns if c != ""]
	if ai_targets and ai_targets[0] in available_columns:
		default_target_index = available_columns.index(ai_targets[0])
		st.success(f"ğŸ¤– AI æ¨èç›®æ ‡åˆ—: {ai_targets[0]}")
	else:
		default_target_index = 0
	
	target = st.selectbox("ç›®æ ‡åˆ—", options=available_columns, index=default_target_index)
	
	# è‡ªåŠ¨æ¨èä»»åŠ¡ç±»å‹
	if target:
		target_series = df[target]
		target_series = target_series.dropna()  # å»é™¤ç¼ºå¤±å€¼
		
		# æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹
		is_numeric = pd.api.types.is_numeric_dtype(target_series)
		unique_count = target_series.nunique()
		total_count = len(target_series)
		
		# æ¨èé€»è¾‘
		if is_numeric and unique_count > 20 and unique_count / total_count > 0.05:
			recommended_task = "regression"
			reason = f"æ•°å€¼ç±»å‹ï¼Œ{unique_count} ä¸ªå”¯ä¸€å€¼"
		else:
			recommended_task = "classification" 
			if not is_numeric:
				reason = f"éæ•°å€¼ç±»å‹ï¼Œ{unique_count} ä¸ªç±»åˆ«"
			else:
				reason = f"æ•°å€¼ç±»å‹ä½†åªæœ‰ {unique_count} ä¸ªå”¯ä¸€å€¼ï¼Œå¯èƒ½æ˜¯åˆ†ç±»"
		
		st.info(f"ğŸ¤– æ¨èä»»åŠ¡ç±»å‹: **{recommended_task}** ({reason})")
		
		# æ˜¾ç¤ºç›®æ ‡å˜é‡çš„åŸºæœ¬ç»Ÿè®¡
		col1, col2 = st.columns(2)
		with col1:
			st.metric("å”¯ä¸€å€¼æ•°é‡", unique_count)
		with col2:
			st.metric("æ ·æœ¬æ•°é‡", total_count)
		
		if unique_count <= 10:
			st.write("ç›®æ ‡å˜é‡çš„å€¼åˆ†å¸ƒ:")
			value_counts = target_series.value_counts().head(10)
			st.bar_chart(value_counts)
	
	# ä»»åŠ¡ç±»å‹é€‰æ‹© - æ™ºèƒ½åº”ç”¨ AI æ¨è
	task_options = ["classification", "regression"]
	if ai_task_type and ai_task_type in task_options:
		default_task_index = task_options.index(ai_task_type)
		st.success(f"ğŸ¤– AI æ¨èä»»åŠ¡ç±»å‹: {ai_task_type}")
	else:
		default_task_index = 0
	
	task_type = st.selectbox("ä»»åŠ¡ç±»å‹", task_options, index=default_task_index)
	
	# ç®—æ³•é€‰æ‹© - æ™ºèƒ½åº”ç”¨ AI æ¨è
	available_algos = ["logreg","rf","xgb","knn","mlp"] if task_type=="classification" else ["linreg","ridge","rf","xgb","knn","mlp"]
	
	# æ˜ å°„ AI æ¨èçš„ç®—æ³•åç§°åˆ°æœ¬åœ°ç®—æ³•åç§°
	algo_mapping = {
		"xgboost": "xgb",
		"random_forest": "rf", 
		"ridge": "ridge",
		"knn": "knn",
		"logistic_regression": "logreg",
		"linear_regression": "linreg",
		"mlp": "mlp"
	}
	
	ai_algos_mapped = []
	if ai_algorithms:
		for ai_algo in ai_algorithms:
			mapped_algo = algo_mapping.get(ai_algo, ai_algo)
			if mapped_algo in available_algos:
				ai_algos_mapped.append(mapped_algo)
		
		if ai_algos_mapped:
			st.success(f"ğŸ¤– AI æ¨èç®—æ³•: {', '.join(ai_algos_mapped)}")
			default_algos = ai_algos_mapped
		else:
			default_algos = ["rf","xgb"]
	else:
		default_algos = ["rf","xgb"]
	
	picked = st.multiselect(
		"å€™é€‰ç®—æ³•",
		available_algos,
		default=default_algos
	)
	budget = st.slider("æ¯æ¨¡å‹æœç´¢æ¬¡æ•° (n_iter)", 10, 80, 30)
	
	# äº¤å‰éªŒè¯æŠ˜æ•° - æ™ºèƒ½åº”ç”¨ AI æ¨è
	default_folds = ai_cv.get("folds", 5) if ai_cv else 5
	if ai_cv and "folds" in ai_cv:
		st.success(f"ğŸ¤– AI æ¨è CV æŠ˜æ•°: {default_folds}")
	
	folds = st.slider("CV æŠ˜æ•°", 3, 10, default_folds)

	if st.button("å¼€å§‹è®­ç»ƒ"):
		X_train, X_test, y_train, y_test, pre, col_info = cleandata.prepare(df, target, task_type)
		leaderboard, artifacts = train_core.run_all(
			X_train, y_train, X_test, y_test,
			task_type=task_type,
			picked_models=picked,
			preprocessor=pre,
			n_iter=budget,
			cv_folds=folds,
			artifacts_dir="artifacts"
		)
		st.success("è®­ç»ƒå®Œæˆï¼")
		st.dataframe(leaderboard)
		st.session_state["__eval_pack__"] = (task_type, X_test, y_test, artifacts)
