import streamlit as st
import os
import json
import sys
sys.path.append(os.path.dirname(__file__))
from llm_agent import detect_task
st.set_page_config(page_title="LLM-Augmented AutoML", layout="wide")
import pandas as pd

st.title("LLM-Augmented AutoML (Local Training)")
st.success("ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸã€‚æ¥ä¸‹æ¥å°†å®ç°æ•°æ®ä¸Šä¼ ã€åˆ¤å®šä¸æŠ¥å‘Šã€‚")

"""å¤šæ–‡ä»¶ä¸Šä¼ åŒº"""
uploaded_files = st.file_uploader("ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ª CSV æ–‡ä»¶", type=["csv"], accept_multiple_files=True)

active_df = None
df_source_name = None
loaded_dfs = {}

if uploaded_files:
	examples_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'examples')
	os.makedirs(examples_dir, exist_ok=True)
	for uf in uploaded_files:
		try:
			df_tmp = pd.read_csv(uf)
			loaded_dfs[uf.name] = df_tmp
			# ä¿å­˜æ–‡ä»¶
			save_path = os.path.join(examples_dir, uf.name)
			with open(save_path, 'wb') as f:
				f.write(uf.getbuffer())
		except Exception as e:
			st.error(f"è¯»å–æ–‡ä»¶ {uf.name} å¤±è´¥: {e}")

	st.success(f"æˆåŠŸè½½å…¥ {len(loaded_dfs)} ä¸ªæ–‡ä»¶ã€‚")

	# -------- æ–°å¢ï¼šæ˜¾ç¤ºå¤šä¸ªæ–‡ä»¶å…±åŒæ‹¥æœ‰çš„åˆ—åï¼ˆå…¬å…±åˆ—ï¼‰ --------
	if len(loaded_dfs) >= 2:
		# è®¡ç®—æ‰€æœ‰æ•°æ®é›†çš„åˆ—é›†åˆäº¤é›†
		list_of_colsets = [set(df.columns) for df in loaded_dfs.values() if hasattr(df, 'columns')]
		if list_of_colsets:  # é˜²å¾¡æ€§æ£€æŸ¥
			common_columns = set.intersection(*list_of_colsets) if len(list_of_colsets) > 1 else list_of_colsets[0]
		else:
			common_columns = set()

		with st.expander("ğŸ“Œ å¤šæ–‡ä»¶å…¬å…±åˆ— (æ‰€æœ‰æ–‡ä»¶éƒ½åŒ…å«)", expanded=True):
			if common_columns:
				st.write(f"å…± {len(common_columns)} ä¸ªå…¬å…±åˆ—ï¼š")
				# æ’åºä¾¿äºæµè§ˆ
				st.code("\n".join(sorted(common_columns)))
			else:
				st.warning("æœªæ‰¾åˆ°æ‰€æœ‰æ–‡ä»¶éƒ½å…±åŒæ‹¥æœ‰çš„åˆ—ã€‚")

			# æç¤ºï¼šå¯é€‰å±•ç¤ºæ¯ä¸ªæ–‡ä»¶ç¼ºå¤±çš„åˆ—ï¼ˆå¸®åŠ©ç”¨æˆ·ç†è§£å·®å¼‚ï¼‰
			show_diff = st.checkbox("æ˜¾ç¤ºå„æ–‡ä»¶ç¼ºå¤±å…¬å…±åˆ—æƒ…å†µ", value=False)
			if show_diff and common_columns:
				for fname, df_tmp in loaded_dfs.items():
					missing_in_file = common_columns - set(df_tmp.columns)
					if missing_in_file:
						st.error(f"{fname} ç¼ºå¤± {len(missing_in_file)} ä¸ªå…¬å…±åˆ—ï¼š{', '.join(sorted(missing_in_file))}")
					else:
						st.success(f"{fname} åŒ…å«å…¨éƒ¨å…¬å…±åˆ— âœ”")

			# ---------------- åˆå¹¶åŠŸèƒ½ï¼ˆæ–°å¢ï¼šæ¨ªå‘åŒ¹é…æ¨¡å¼ï¼‰ ----------------
			st.markdown("---")
			st.markdown("### ğŸ”— åˆå¹¶å·¥å…·")
			merge_mode = st.radio(
				"é€‰æ‹©åˆå¹¶æ–¹å¼",
				["çºµå‘å †å ï¼ˆä»…å…¬å…±åˆ—ï¼‰", "æ¨ªå‘åŒ¹é…ï¼ˆå…¬å…±åˆ—ä½œä¸ºé”®ï¼Œåˆå¹¶å…¶ä½™åˆ—ï¼‰"],
				index=0,
				help="æ¨ªå‘åŒ¹é…=ç±»ä¼¼å¤šè¡¨ joinï¼›çºµå‘å †å =append è¡Œã€‚"
			)

			if merge_mode.startswith("çºµå‘"):
				st.caption("ä»…ä¿ç•™å…¬å…±åˆ—å¹¶æŒ‰è¡Œå †å ï¼ˆä¹‹å‰çš„è¡Œä¸ºï¼‰ã€‚")
				add_source_col = st.checkbox("æ·»åŠ æ¥æºæ–‡ä»¶åˆ— (_source_file)", value=True, key="add_source_file")
				merge_btn = st.button("âš™ï¸ æ‰§è¡Œçºµå‘åˆå¹¶", key="merge_vertical")
				if merge_btn:
					if not common_columns:
						st.error("æ— æ³•åˆå¹¶ï¼šæ²¡æœ‰å…¬å…±åˆ—ã€‚")
					else:
						try:
							merged_parts = []
							for fname, df_part in loaded_dfs.items():
								subset = df_part[list(common_columns)].copy()
								if add_source_col:
									subset["_source_file"] = fname
								merged_parts.append(subset)
							merged_df = pd.concat(merged_parts, ignore_index=True)
							base_name = "merged_common.csv"
							final_name = base_name
							idx = 1
							while final_name in loaded_dfs:
								idx += 1
								final_name = f"merged_common_{idx}.csv"
							try:
								examples_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'examples')
								os.makedirs(examples_dir, exist_ok=True)
								merged_path = os.path.join(examples_dir, final_name)
								merged_df.to_csv(merged_path, index=False, encoding="utf-8")
							except Exception as fs_err:
								st.warning(f"åˆå¹¶æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼Œä½†å†…å­˜ä¾æ—§å¯ç”¨ï¼š{fs_err}")
							loaded_dfs[final_name] = merged_df
							st.session_state["merged_common_df"] = merged_df
							st.success(f"çºµå‘åˆå¹¶æˆåŠŸï¼š{final_name}ï¼Œå½¢çŠ¶ {merged_df.shape}")
							csv_bytes = merged_df.to_csv(index=False).encode('utf-8')
							st.download_button("â¬‡ï¸ ä¸‹è½½ç»“æœ", data=csv_bytes, file_name=final_name, mime="text/csv")
							st.info("åœ¨ä¸‹æ–¹æ–‡ä»¶é€‰æ‹©æ¡†ä¸­å¯é€‰æ‹©è¯¥åˆå¹¶æ–‡ä»¶ç»§ç»­åˆ†æã€‚")
						except Exception as merge_err:
							st.error(f"åˆå¹¶å¤±è´¥ï¼š{merge_err}")

			else:  # æ¨ªå‘åŒ¹é…
				st.caption("ä½¿ç”¨å…¬å…±åˆ—ä½œä¸ºé”®åšå¤šè¡¨ joinï¼Œä¿ç•™æ¯ä¸ªæ–‡ä»¶çš„å…¶ä½™åˆ—ã€‚")
				if not common_columns:
					st.error("æ— æ³•è¿›è¡Œæ¨ªå‘åŒ¹é…ï¼šæ²¡æœ‰å…¬å…±åˆ—ã€‚")
				else:
					key_cols = sorted(common_columns)
					st.info(f"é”®åˆ—ï¼š{', '.join(key_cols)}")
					join_type = st.selectbox("Join ç±»å‹", ["outer", "inner", "left"], index=0, help="outer=ä¿ç•™æ‰€æœ‰é”®; inner=ä»…å…¬å…±é”®; left=ä»¥ç¬¬ä¸€ä¸ªæ–‡ä»¶ä¸ºä¸»è¡¨")
					prefix_cols = st.checkbox("ä¸ºéé”®åˆ—åŠ æ–‡ä»¶åå‰ç¼€ä»¥é˜²å†²çª", value=True, key="prefix_non_key")
					drop_dup = st.checkbox("å¦‚æœæŸæ–‡ä»¶é”®åˆ—æœ‰é‡å¤è¡Œï¼Œä»…ä¿ç•™ç¬¬ä¸€æ¡", value=True, key="drop_dup_keys")
					btn_hmerge = st.button("âš™ï¸ æ‰§è¡Œæ¨ªå‘åŒ¹é…åˆå¹¶", key="merge_horizontal")
					if btn_hmerge:
						try:
							merged_df = None
							for idx_file, (fname, df_part) in enumerate(loaded_dfs.items()):
								work_df = df_part.copy()
								missing_keys = [k for k in key_cols if k not in work_df.columns]
								if missing_keys:
									st.error(f"æ–‡ä»¶ {fname} ç¼ºå¤±é”®åˆ— {missing_keys}ï¼Œè·³è¿‡ã€‚")
									continue
								# å¤„ç†é‡å¤é”®
								if drop_dup and work_df.duplicated(subset=key_cols).any():
									dup_count = work_df.duplicated(subset=key_cols).sum()
									st.warning(f"{fname} é”®åˆ—å­˜åœ¨ {dup_count} ä¸ªé‡å¤ï¼Œå°†ä¿ç•™ç¬¬ä¸€æ¡ã€‚")
									work_df = work_df.drop_duplicates(subset=key_cols, keep='first')
								non_key_cols = [c for c in work_df.columns if c not in key_cols]
								if prefix_cols:
									base_prefix = os.path.splitext(os.path.basename(fname))[0]
									rename_map = {c: f"{base_prefix}__{c}" for c in non_key_cols}
									work_df = work_df.rename(columns=rename_map)
								cols_to_use = key_cols + [c for c in work_df.columns if c not in key_cols]
								if merged_df is None:
									merged_df = work_df[cols_to_use]
								else:
									merged_df = pd.merge(merged_df, work_df[cols_to_use], on=key_cols, how=join_type)
							if merged_df is None:
								st.error("æœªèƒ½ç”Ÿæˆåˆå¹¶ç»“æœï¼ˆå¯èƒ½æ‰€æœ‰æ–‡ä»¶éƒ½è¢«è·³è¿‡ï¼‰")
							else:
								base_name = "merged_horizontal.csv"
								final_name = base_name
								ix = 1
								while final_name in loaded_dfs:
									ix += 1
									final_name = f"merged_horizontal_{ix}.csv"
								try:
									examples_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'examples')
									os.makedirs(examples_dir, exist_ok=True)
									merged_path = os.path.join(examples_dir, final_name)
									merged_df.to_csv(merged_path, index=False, encoding='utf-8')
								except Exception as fs_err:
									st.warning(f"åˆå¹¶æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼Œä½†å†…å­˜ä»å¯ä½¿ç”¨ï¼š{fs_err}")
								loaded_dfs[final_name] = merged_df
								st.success(f"æ¨ªå‘åŒ¹é…åˆå¹¶æˆåŠŸï¼š{final_name}ï¼Œå½¢çŠ¶ {merged_df.shape}")
								csv_bytes = merged_df.to_csv(index=False).encode('utf-8')
								st.download_button("â¬‡ï¸ ä¸‹è½½ç»“æœ", data=csv_bytes, file_name=final_name, mime="text/csv")
								st.info("åœ¨ä¸‹æ–¹æ–‡ä»¶é€‰æ‹©æ¡†ä¸­å¯é€‰æ‹©è¯¥æ¨ªå‘åˆå¹¶æ–‡ä»¶ç»§ç»­åˆ†æã€‚")
						except Exception as e:
							st.error(f"æ¨ªå‘åˆå¹¶å¤±è´¥ï¼š{e}")
	else:
		st.info("ä¸Šä¼  2 ä¸ªåŠä»¥ä¸Šæ–‡ä»¶åï¼Œå°†åœ¨æ­¤æ˜¾ç¤ºå®ƒä»¬çš„å…¬å…±åˆ—ã€‚")

	file_names = list(loaded_dfs.keys())
	pick_name = st.selectbox("é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶è¿›è¡Œé¢„è§ˆä¸åˆ†æ", file_names)
	active_df = loaded_dfs.get(pick_name)
	df_source_name = pick_name

if active_df is not None:
	df = active_df  # ä¿æŒåç»­ä»£ç å˜é‡åä¸å˜
	st.info(f"å½“å‰æ´»åŠ¨æ•°æ®é›†: {df_source_name}; å½¢çŠ¶: {df.shape}")
	st.write("æ•°æ®é¢„è§ˆï¼š")
	st.dataframe(df.head())

	#ï¼ˆå·²ç§»é™¤å¤šæ–‡ä»¶ä¸»åˆ—åŒ¹é…åŠŸèƒ½ï¼‰
	with st.expander("ï¿½ğŸ” æ•°æ®æ¦‚è§ˆ", expanded=False):
		st.write("æ•°æ®æè¿°ï¼š")
		st.write(df.describe(include='all').transpose())
		st.write("ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š")
		st.write(df.isnull().sum())

	# ----------- åˆ¤å®šæŒ‰é’®ä¸ç»“æœå±•ç¤ºåŒºå— -------------
	# æ„å»ºç®€æ˜“ profileï¼›åç»­å¯æ›¿æ¢ä¸º ingest.profile
	prof = {
		"columns": [
			{"name": c, "dtype": str(df[c].dtype), "missing": int(df[c].isnull().sum()), "unique": int(df[c].nunique())}
			for c in df.columns
		]
	}

	user_question = st.text_area("ä½ çš„é—®é¢˜ï¼ˆå¯é€‰ï¼‰", placeholder="ä¾‹å¦‚ï¼šæˆ‘ä»¬èƒ½å¦é¢„æµ‹ä¹˜å®¢æ˜¯å¦ç”Ÿè¿˜ï¼Ÿæˆ– é¢„æµ‹ä»·æ ¼/åˆ†ç¾¤ç­‰ã€‚")

	col1, col2, col3 = st.columns([1,1,1])
	with col1:
		if st.button("ğŸ” å‘ç°ç ”ç©¶é—®é¢˜"):
			with st.spinner("AI æ­£åœ¨åˆ†ææ•°æ®ï¼Œå¯»æ‰¾æœ‰ä»·å€¼çš„ç ”ç©¶é—®é¢˜..."):
				from llm_agent import suggest_research_questions
				research_suggestions = suggest_research_questions(prof)
			st.session_state["research_suggestions"] = research_suggestions
			st.success("é—®é¢˜å‘ç°å®Œæˆï¼")
	
	with col2:
		if st.button("ğŸ¤– æ™ºèƒ½åˆ¤å®šä»»åŠ¡"):
			with st.spinner("è°ƒç”¨ OpenAI åˆ¤å®šä»»åŠ¡ç±»å‹ä¸æ–¹æ¡ˆ..."):
				plan = detect_task(prof, user_question or "")
			st.session_state["plan"] = plan
			st.success("ä»»åŠ¡åˆ¤å®šå®Œæˆï¼")
	
	with col3:
		if "plan" in st.session_state:
			st.download_button("ğŸ“„ ä¸‹è½½åˆ¤å®šç»“æœ", data=json.dumps(st.session_state["plan"], ensure_ascii=False, indent=2),
							   file_name="task_plan.json", mime="application/json")

	# æ˜¾ç¤ºç ”ç©¶é—®é¢˜å»ºè®®
	if "research_suggestions" in st.session_state:
		st.subheader("ğŸ” AI æ•°æ®æ´å¯Ÿï¼šå¯ç ”ç©¶çš„é—®é¢˜")
		
		suggestions = st.session_state["research_suggestions"]
		
		def display_research_suggestions(suggestions):
			"""å¯è¯»åŒ–æ˜¾ç¤ºç ”ç©¶é—®é¢˜å»ºè®®"""
			
			# æ£€æŸ¥ suggestions æ˜¯å¦ä¸ºå­—å…¸
			if not isinstance(suggestions, dict):
				st.error("âŒ ç ”ç©¶å»ºè®®æ•°æ®æ ¼å¼é”™è¯¯")
				st.json(suggestions)
				return
			
			# ç ”ç©¶é—®é¢˜
			questions = suggestions.get("research_questions", [])
			if questions:
				st.markdown("### ğŸ’¡ **æ¨èç ”ç©¶é—®é¢˜**")
				
				for i, q in enumerate(questions):
					with st.expander(f"ğŸ“‹ é—®é¢˜ {i+1}: {q.get('question', 'æœªçŸ¥é—®é¢˜')}", expanded=i==0):
						col1, col2 = st.columns(2)
						with col1:
							st.markdown(f"**ç±»å‹**: {q.get('type', 'æœªçŸ¥')}")
							st.markdown(f"**éš¾åº¦**: {q.get('difficulty', 'æœªçŸ¥')}")
						with col2:
							if q.get('target_column'):
								st.markdown(f"**ç›®æ ‡åˆ—**: `{q.get('target_column')}`")
							methods = q.get('required_methods', [])
							if methods:
								st.markdown(f"**æ¨èæ–¹æ³•**: {', '.join(methods)}")
						
						st.markdown("**å•†ä¸šä»·å€¼**:")
						st.info(q.get('business_value', 'æœªæä¾›'))
			
			# åº”ç”¨åœºæ™¯
			scenarios = suggestions.get("application_scenarios", [])
			if scenarios:
				st.markdown("### ğŸ¯ **åº”ç”¨åœºæ™¯**")
				for i, scenario in enumerate(scenarios):
					st.markdown(f"**{i+1}.** {scenario}")
			
			# å…³é”®æ´å¯Ÿæ½œåŠ›
			insights = suggestions.get("key_insights_potential", [])
			if insights:
				st.markdown("### ğŸ”® **å¯èƒ½å‘ç°çš„æ´å¯Ÿ**")
				for insight in insights:
					st.markdown(f"â€¢ {insight}")
			
			# æ•°æ®é›†ä¼˜åŠ¿
			strengths = suggestions.get("dataset_strengths", [])
			if strengths:
				st.markdown("### âœ¨ **æ•°æ®é›†ä¼˜åŠ¿**")
				for strength in strengths:
					st.markdown(f"âœ… {strength}")
			
			# é™åˆ¶å’Œæ³¨æ„äº‹é¡¹
			limitations = suggestions.get("limitations", [])
			if limitations:
				st.markdown("### âš ï¸ **æ³¨æ„äº‹é¡¹**")
				for limitation in limitations:
					st.warning(f"âš ï¸ {limitation}")
			
			# å»ºè®®
			recommendations = suggestions.get("recommendations", {})
			if recommendations:
				st.markdown("### ğŸ¯ **è¡ŒåŠ¨å»ºè®®**")
				
				# æ£€æŸ¥ recommendations æ˜¯å¦ä¸ºå­—å…¸
				if isinstance(recommendations, dict):
					priority = recommendations.get("priority_questions", [])
					if priority:
						st.markdown("**ğŸ”¥ ä¼˜å…ˆç ”ç©¶é—®é¢˜**:")
						for p in priority:
							st.markdown(f"â€¢ {p}")
					
					next_steps = recommendations.get("next_steps", [])
					if next_steps:
						st.markdown("**ğŸ“‹ å»ºè®®æ­¥éª¤**:")
						for step in next_steps:
							st.markdown(f"â€¢ {step}")
					
					additional_data = recommendations.get("additional_data", [])
					if additional_data:
						st.markdown("**ğŸ“Š å¯èƒ½éœ€è¦çš„é¢å¤–æ•°æ®**:")
						for data in additional_data:
							st.markdown(f"â€¢ {data}")
				else:
					# å¦‚æœ recommendations æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥æ˜¾ç¤º
					st.markdown(f"**ğŸ’¡ å»ºè®®**: {recommendations}")
		
		display_research_suggestions(suggestions)
		
		# åŸå§‹ JSON (å¯é€‰)
		with st.expander("ğŸ” æŸ¥çœ‹è¯¦ç»†åˆ†æç»“æœ (JSON)", expanded=False):
			st.json(suggestions)
		
		st.success("ğŸ’¡ åŸºäºä»¥ä¸Šåˆ†æï¼Œä½ å¯ä»¥é€‰æ‹©æ„Ÿå…´è¶£çš„é—®é¢˜è¿›è¡Œæ·±å…¥ç ”ç©¶ï¼")

	if "plan" in st.session_state:
		st.subheader("ğŸ¤– AI æ™ºèƒ½åˆ¤å®šç»“æœ")
		
		plan = st.session_state["plan"]
		
		# å¯è¯»åŒ–æ˜¾ç¤ºåˆ¤å®šç»“æœ
		def display_readable_plan(plan):
			# ä»»åŠ¡ç±»å‹
			task_type = plan.get("task_type", "æœªçŸ¥")
			task_type_cn = {"classification": "åˆ†ç±»ä»»åŠ¡", "regression": "å›å½’ä»»åŠ¡", "clustering": "èšç±»ä»»åŠ¡"}.get(task_type, task_type)
			
			st.markdown(f"### ğŸ“Š **ä»»åŠ¡ç±»å‹**: {task_type_cn}")
			if task_type == "classification":
				st.info("ğŸ¯ è¿™æ˜¯ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯é¢„æµ‹ç¦»æ•£çš„ç±»åˆ«æ ‡ç­¾")
			elif task_type == "regression":
				st.info("ğŸ“ˆ è¿™æ˜¯ä¸€ä¸ªå›å½’ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯é¢„æµ‹è¿ç»­çš„æ•°å€¼")
			elif task_type == "clustering":
				st.info("ğŸ” è¿™æ˜¯ä¸€ä¸ªèšç±»ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯å‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼")
			
			# ç›®æ ‡å€™é€‰åˆ—
			targets = plan.get("target_candidates", [])
			if targets:
				st.markdown("### ğŸ¯ **æ¨èç›®æ ‡åˆ—**")
				for i, target in enumerate(targets):
					st.markdown(f"**{i+1}.** `{target}`")
			else:
				st.warning("âš ï¸ æœªæ‰¾åˆ°æ˜ç¡®çš„ç›®æ ‡åˆ—ï¼Œè¯·æ‰‹åŠ¨é€‰æ‹©")
			
			# æ¨èç®—æ³•
			algorithms = plan.get("algorithms", [])
			if algorithms:
				st.markdown("### ğŸ¤– **æ¨èç®—æ³•**")
				algo_names = {
					"xgboost": "XGBoost (æç«¯æ¢¯åº¦æå‡)",
					"ridge": "Ridge å›å½’ (å²­å›å½’)",
					"knn": "K-è¿‘é‚»ç®—æ³•",
					"random_forest": "éšæœºæ£®æ—",
					"linear_regression": "çº¿æ€§å›å½’",
					"logistic_regression": "é€»è¾‘å›å½’",
					"svm": "æ”¯æŒå‘é‡æœº",
					"mlp": "å¤šå±‚æ„ŸçŸ¥æœº"
				}
				
				cols = st.columns(min(len(algorithms), 3))
				for i, algo in enumerate(algorithms):
					with cols[i % 3]:
						algo_display = algo_names.get(algo, algo.replace('_', ' ').title())
						st.markdown(f"**{i+1}.** {algo_display}")
			
			# è¯„ä¼°æŒ‡æ ‡
			metrics = plan.get("metrics", [])
			if metrics:
				st.markdown("### ğŸ“ **è¯„ä¼°æŒ‡æ ‡**")
				metric_names = {
					"rmse": "RMSE (å‡æ–¹æ ¹è¯¯å·®)",
					"mae": "MAE (å¹³å‡ç»å¯¹è¯¯å·®)", 
					"r2": "RÂ² (å†³å®šç³»æ•°)",
					"accuracy": "å‡†ç¡®ç‡",
					"f1": "F1 åˆ†æ•°",
					"precision": "ç²¾ç¡®ç‡",
					"recall": "å¬å›ç‡",
					"auc": "AUC (æ›²çº¿ä¸‹é¢ç§¯)"
				}
				
				metric_cols = st.columns(min(len(metrics), 3))
				for i, metric in enumerate(metrics):
					with metric_cols[i % 3]:
						metric_display = metric_names.get(metric, metric.upper())
						st.markdown(f"**â€¢** {metric_display}")
			
			# äº¤å‰éªŒè¯è®¾ç½®
			cv_info = plan.get("cv", {})
			if cv_info:
				st.markdown("### âœ… **äº¤å‰éªŒè¯è®¾ç½®**")
				folds = cv_info.get("folds", 5)
				stratified = cv_info.get("stratified", False)
				
				col1, col2 = st.columns(2)
				with col1:
					st.metric("äº¤å‰éªŒè¯æŠ˜æ•°", f"{folds} æŠ˜")
				with col2:
					stratify_text = "æ˜¯" if stratified else "å¦"
					st.metric("åˆ†å±‚é‡‡æ ·", stratify_text)
			
			# ç±»åˆ«ä¸å¹³è¡¡ä¿¡æ¯
			imbalance = plan.get("imbalance", {})
			if imbalance and imbalance.get("is_imbalanced"):
				st.markdown("### âš–ï¸ **æ•°æ®ä¸å¹³è¡¡è­¦å‘Š**")
				ratio = imbalance.get("ratio")
				if ratio:
					st.warning(f"æ£€æµ‹åˆ°æ•°æ®ä¸å¹³è¡¡ï¼Œä¸»è¦ç±»åˆ«å æ¯”: {ratio:.1%}")
					st.caption("å»ºè®®è€ƒè™‘ä½¿ç”¨ç±»åˆ«æƒé‡å¹³è¡¡æˆ–é‡‡æ ·æŠ€æœ¯")
		
		# æ˜¾ç¤ºå¯è¯»åŒ–ç»“æœ
		display_readable_plan(plan)
		
		# å¯é€‰æ˜¾ç¤ºåŸå§‹ JSON
		with st.expander("ğŸ” æŸ¥çœ‹è¯¦ç»† JSON ç»“æœ", expanded=False):
			st.json(plan)
		
		st.caption("ğŸ’¡ ä»¥ä¸Šç»“æœå°†è‡ªåŠ¨åº”ç”¨åˆ°è®­ç»ƒè®¾ç½®ä¸­ï¼Œä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨è°ƒæ•´å‚æ•°ã€‚")

	# ------------------ è®­ç»ƒè®¾ç½®ä¸è®­ç»ƒæµç¨‹ï¼ˆæœ€å°æ¥å…¥ï¼‰ ------------------
	import sys, os
	sys.path.append(os.path.dirname(os.path.dirname(__file__)))
	from core import cleandata, train as train_core

	st.subheader("ğŸ› ï¸ è®­ç»ƒè®¾ç½®ï¼ˆæœ¬åœ°ï¼‰")
	
	# æ™ºèƒ½åº”ç”¨ AI åˆ¤å®šç»“æœ
	plan = st.session_state.get("plan", {})
	ai_targets = plan.get("target_candidates", [])
	ai_task_type = plan.get("task_type", "")
	ai_algorithms = plan.get("algorithms", [])
	ai_cv = plan.get("cv", {})
	
	# ç›®æ ‡åˆ—é€‰æ‹© - ä¼˜å…ˆä½¿ç”¨ AI æ¨è
	available_columns = [c for c in df.columns if c != ""]
	if ai_targets and ai_targets[0] in available_columns:
		default_target_index = available_columns.index(ai_targets[0])
		st.success(f"ğŸ¤– AI æ¨èç›®æ ‡åˆ—: {ai_targets[0]}")
	else:
		default_target_index = 0
	
	target = st.selectbox("ç›®æ ‡åˆ—", options=available_columns, index=default_target_index)
	
	# è‡ªåŠ¨æ¨èä»»åŠ¡ç±»å‹
	if target:
		target_series = df[target]
		target_series = target_series.dropna()  # å»é™¤ç¼ºå¤±å€¼
		
		# æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹
		is_numeric = pd.api.types.is_numeric_dtype(target_series)
		unique_count = target_series.nunique()
		total_count = len(target_series)
		
		# æ¨èé€»è¾‘
		if is_numeric and unique_count > 20 and unique_count / total_count > 0.05:
			recommended_task = "regression"
			reason = f"æ•°å€¼ç±»å‹ï¼Œ{unique_count} ä¸ªå”¯ä¸€å€¼"
		else:
			recommended_task = "classification" 
			if not is_numeric:
				reason = f"éæ•°å€¼ç±»å‹ï¼Œ{unique_count} ä¸ªç±»åˆ«"
			else:
				reason = f"æ•°å€¼ç±»å‹ä½†åªæœ‰ {unique_count} ä¸ªå”¯ä¸€å€¼ï¼Œå¯èƒ½æ˜¯åˆ†ç±»"
		
		st.info(f"ğŸ¤– æ¨èä»»åŠ¡ç±»å‹: **{recommended_task}** ({reason})")
		
		# æ˜¾ç¤ºç›®æ ‡å˜é‡çš„åŸºæœ¬ç»Ÿè®¡
		col1, col2 = st.columns(2)
		with col1:
			st.metric("å”¯ä¸€å€¼æ•°é‡", unique_count)
		with col2:
			st.metric("æ ·æœ¬æ•°é‡", total_count)
		
		if unique_count <= 10:
			st.write("ç›®æ ‡å˜é‡çš„å€¼åˆ†å¸ƒ:")
			value_counts = target_series.value_counts().head(10)
			st.bar_chart(value_counts)
	
	# ä»»åŠ¡ç±»å‹é€‰æ‹© - æ™ºèƒ½åº”ç”¨ AI æ¨è
	task_options = ["classification", "regression"]
	if ai_task_type and ai_task_type in task_options:
		default_task_index = task_options.index(ai_task_type)
		st.success(f"ğŸ¤– AI æ¨èä»»åŠ¡ç±»å‹: {ai_task_type}")
	else:
		default_task_index = 0
	
	task_type = st.selectbox("ä»»åŠ¡ç±»å‹", task_options, index=default_task_index)
	
	# ç®—æ³•é€‰æ‹© - æ™ºèƒ½åº”ç”¨ AI æ¨è
	available_algos = ["logreg","rf","xgb","knn","mlp"] if task_type=="classification" else ["linreg","ridge","rf","xgb","knn","mlp"]
	
	# æ˜ å°„ AI æ¨èçš„ç®—æ³•åç§°åˆ°æœ¬åœ°ç®—æ³•åç§°
	algo_mapping = {
		"xgboost": "xgb",
		"random_forest": "rf", 
		"ridge": "ridge",
		"knn": "knn",
		"logistic_regression": "logreg",
		"linear_regression": "linreg",
		"mlp": "mlp"
	}
	
	ai_algos_mapped = []
	if ai_algorithms:
		for ai_algo in ai_algorithms:
			mapped_algo = algo_mapping.get(ai_algo, ai_algo)
			if mapped_algo in available_algos:
				ai_algos_mapped.append(mapped_algo)
		
		if ai_algos_mapped:
			st.success(f"ğŸ¤– AI æ¨èç®—æ³•: {', '.join(ai_algos_mapped)}")
			default_algos = ai_algos_mapped
		else:
			default_algos = ["rf","xgb"]
	else:
		default_algos = ["rf","xgb"]
	
	picked = st.multiselect(
		"å€™é€‰ç®—æ³•",
		available_algos,
		default=default_algos
	)
	budget = st.slider("æ¯æ¨¡å‹æœç´¢æ¬¡æ•° (n_iter)", 10, 80, 30)
	
	# äº¤å‰éªŒè¯æŠ˜æ•° - æ™ºèƒ½åº”ç”¨ AI æ¨è
	default_folds = ai_cv.get("folds", 5) if ai_cv else 5
	if ai_cv and "folds" in ai_cv:
		st.success(f"ğŸ¤– AI æ¨è CV æŠ˜æ•°: {default_folds}")
	
	folds = st.slider("CV æŠ˜æ•°", 3, 10, default_folds)


	# è¯„ä¼°è¡Œæ•°é™åˆ¶è®¾ç½®
	with st.expander("âš™ï¸ è¯„ä¼°æ•°æ®é‡è®¾ç½®", expanded=False):
		col_a, col_b = st.columns([1,2])
		with col_a:
			use_eval_limit = st.checkbox("é™åˆ¶è¯„ä¼°è¡Œæ•°", value=True, help="ä»…åœ¨è¯„ä¼°æŒ‡æ ‡/é¢„æµ‹æ—¶ä½¿ç”¨æµ‹è¯•é›†å‰ N è¡Œï¼Œé€‚åˆå¿«é€Ÿè¿­ä»£ã€‚")
		with col_b:
			if use_eval_limit:
				custom_eval_rows = st.number_input("è¯„ä¼°æœ€å¤§è¡Œæ•° N", min_value=50, max_value=20000, value=500, step=50, help="è¶…è¿‡è¯¥è¡Œæ•°æ—¶ä»…æˆªå–å‰ N è¡Œï¼›ä¸å½±å“æ¨¡å‹è®­ç»ƒã€‚")
			else:
				custom_eval_rows = None

	if st.button("å¼€å§‹è®­ç»ƒ"):
		X_train, X_test, y_train, y_test, pre, col_info = cleandata.prepare(df, target, task_type)
		leaderboard, artifacts = train_core.run_all(
			X_train, y_train, X_test, y_test,
			task_type=task_type,
			picked_models=picked,
			preprocessor=pre,
			n_iter=budget,
			cv_folds=folds,
			artifacts_dir="artifacts"
		)
		st.success("è®­ç»ƒå®Œæˆï¼")
		st.dataframe(leaderboard)
		# æ ¹æ®ç”¨æˆ·è®¾ç½®é™åˆ¶è¯„ä¼°é˜¶æ®µä½¿ç”¨çš„æµ‹è¯•é›†è¡Œæ•°
		if custom_eval_rows and custom_eval_rows > 0 and len(X_test) > custom_eval_rows:
			n_eval = int(min(custom_eval_rows, len(X_test)))
			if hasattr(X_test, 'head'):
				try:
					X_test_eval = X_test.head(n_eval)
				except Exception:
					X_test_eval = X_test[:n_eval]
			else:
				X_test_eval = X_test[:n_eval]
			if hasattr(y_test, 'iloc'):
				y_test_eval = y_test.iloc[:n_eval]
			else:
				y_test_eval = y_test[:n_eval]
			st.info(f"è¯„ä¼°è¡Œæ•°é™åˆ¶å¯ç”¨ï¼šä½¿ç”¨æµ‹è¯•é›†å‰ {n_eval} è¡Œï¼ˆåŸå§‹ {len(X_test)} è¡Œï¼‰ã€‚")
		else:
			X_test_eval = X_test
			y_test_eval = y_test
			st.info(f"è¯„ä¼°è¡Œæ•°é™åˆ¶æœªå¯ç”¨ï¼Œä½¿ç”¨å…¨éƒ¨æµ‹è¯•é›† {len(X_test)} è¡Œã€‚")
		st.session_state["__eval_pack__"] = (task_type, X_test_eval, y_test_eval, artifacts)
