import streamlit as st
import os
import shutil
import json
import sys
sys.path.append(os.path.dirname(__file__))
from llm_agent import detect_task
st.set_page_config(page_title="LLM-Augmented AutoML", layout="wide")
import pandas as pd

st.title("LLM-Augmented AutoML (Local Training)")
st.success("ç¯å¢ƒåˆå§‹åŒ–æˆåŠŸã€‚æ¥ä¸‹æ¥å°†å®ç°æ•°æ®ä¸Šä¼ ã€åˆ¤å®šä¸æŠ¥å‘Šã€‚")

# ä¼šè¯é¦–æ¬¡è¿è¡Œæ—¶æ¸…ç©º examples ç›®å½•ï¼ˆé¿å…æ¯æ¬¡ rerun éƒ½æ¸…ç©ºæ–°ä¸Šä¼ æ–‡ä»¶ï¼‰
try:
	if not st.session_state.get("__examples_cleared__", False):
		project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
		examples_dir = os.path.join(project_root, 'examples')
		if os.path.exists(examples_dir):
			for name in os.listdir(examples_dir):
				p = os.path.join(examples_dir, name)
				try:
					if os.path.isfile(p) or os.path.islink(p):
						os.remove(p)
					elif os.path.isdir(p):
						shutil.rmtree(p)
				except Exception:
					# å•ä¸ªæ–‡ä»¶å¤±è´¥ä¸å½±å“æ•´ä½“
					pass
		st.session_state["__examples_cleared__"] = True
		st.caption("å·²æŒ‰éœ€æ±‚æ¸…ç©º examples æ–‡ä»¶å¤¹ï¼ˆæœ¬ä¼šè¯ä»…ä¸€æ¬¡ï¼‰ã€‚")
except Exception as __clear_err:
	st.warning(f"æ¸…ç©º examples æ–‡ä»¶å¤¹å¤±è´¥ï¼š{__clear_err}")

"""å¤šæ–‡ä»¶ä¸Šä¼ åŒº"""
uploaded_files = st.file_uploader("ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ª CSV æ–‡ä»¶", type=["csv"], accept_multiple_files=True)

active_df = None
df_source_name = None
loaded_dfs = {}

if uploaded_files:
	examples_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'examples')
	os.makedirs(examples_dir, exist_ok=True)
	for uf in uploaded_files:
		try:
			df_tmp = pd.read_csv(uf)
			loaded_dfs[uf.name] = df_tmp
			# ä¿å­˜æ–‡ä»¶
			save_path = os.path.join(examples_dir, uf.name)
			with open(save_path, 'wb') as f:
				f.write(uf.getbuffer())
		except Exception as e:
			st.error(f"è¯»å–æ–‡ä»¶ {uf.name} å¤±è´¥: {e}")

	st.success(f"æˆåŠŸè½½å…¥ {len(loaded_dfs)} ä¸ªæ–‡ä»¶ã€‚")

	# é¡¶éƒ¨å ä½ï¼šå°†â€œæ–‡ä»¶é¢„è§ˆä¸åˆ†æâ€ç§»åŠ¨åˆ°åˆå¹¶åŠŸèƒ½ä¸Šæ–¹æ˜¾ç¤º
	select_container = st.container()
	# é¡¶éƒ¨å ä½ï¼šå°†â€œæ•°æ®é¢„è§ˆ/æ•°æ®æ¦‚è§ˆâ€ä¹Ÿç§»åŠ¨åˆ°åˆå¹¶åŠŸèƒ½ä¸Šæ–¹æ˜¾ç¤º
	preview_container = st.container()

	# -------- æ–°å¢ï¼šæ˜¾ç¤ºå¤šä¸ªæ–‡ä»¶å…±åŒæ‹¥æœ‰çš„åˆ—åï¼ˆå…¬å…±åˆ—ï¼‰ --------
	if len(loaded_dfs) >= 2:
		# è®¡ç®—æ‰€æœ‰æ•°æ®é›†çš„åˆ—é›†åˆäº¤é›†
		list_of_colsets = [set(df.columns) for df in loaded_dfs.values() if hasattr(df, 'columns')]
		if list_of_colsets:  # é˜²å¾¡æ€§æ£€æŸ¥
			common_columns = set.intersection(*list_of_colsets) if len(list_of_colsets) > 1 else list_of_colsets[0]
		else:
			common_columns = set()

		with st.expander("ğŸ“Œ å¤šæ–‡ä»¶å…¬å…±åˆ— (æ‰€æœ‰æ–‡ä»¶éƒ½åŒ…å«)", expanded=True):
			if common_columns:
				st.write(f"å…± {len(common_columns)} ä¸ªå…¬å…±åˆ—ï¼š")
				# æ’åºä¾¿äºæµè§ˆ
				st.code("\n".join(sorted(common_columns)))
			else:
				st.warning("æœªæ‰¾åˆ°æ‰€æœ‰æ–‡ä»¶éƒ½å…±åŒæ‹¥æœ‰çš„åˆ—ã€‚")

			# æç¤ºï¼šå¯é€‰å±•ç¤ºæ¯ä¸ªæ–‡ä»¶ç¼ºå¤±çš„åˆ—ï¼ˆå¸®åŠ©ç”¨æˆ·ç†è§£å·®å¼‚ï¼‰
			show_diff = st.checkbox("æ˜¾ç¤ºå„æ–‡ä»¶ç¼ºå¤±å…¬å…±åˆ—æƒ…å†µ", value=False)
			if show_diff and common_columns:
				for fname, df_tmp in loaded_dfs.items():
					missing_in_file = common_columns - set(df_tmp.columns)
					if missing_in_file:
						st.error(f"{fname} ç¼ºå¤± {len(missing_in_file)} ä¸ªå…¬å…±åˆ—ï¼š{', '.join(sorted(missing_in_file))}")
					else:
						st.success(f"{fname} åŒ…å«å…¨éƒ¨å…¬å…±åˆ— âœ”")

			# ---------------- åˆå¹¶åŠŸèƒ½ï¼ˆæ–°å¢ï¼šæ¨ªå‘åŒ¹é…æ¨¡å¼ï¼‰ ----------------
			st.markdown("---")
			st.markdown("### ğŸ”— åˆå¹¶å·¥å…·")
			merge_mode = st.radio(
				"é€‰æ‹©åˆå¹¶æ–¹å¼",
				["çºµå‘å †å ï¼ˆä»…å…¬å…±åˆ—ï¼‰", "æ¨ªå‘åŒ¹é…ï¼ˆå…¬å…±åˆ—ä½œä¸ºé”®ï¼Œåˆå¹¶å…¶ä½™åˆ—ï¼‰"],
				index=0,
				help="æ¨ªå‘åŒ¹é…=ç±»ä¼¼å¤šè¡¨ joinï¼›çºµå‘å †å =append è¡Œã€‚"
			)

			if merge_mode.startswith("çºµå‘"):
				st.caption("ä»…ä¿ç•™å…¬å…±åˆ—å¹¶æŒ‰è¡Œå †å ï¼ˆä¹‹å‰çš„è¡Œä¸ºï¼‰ã€‚")
				add_source_col = st.checkbox("æ·»åŠ æ¥æºæ–‡ä»¶åˆ— (_source_file)", value=True, key="add_source_file")
				merge_btn = st.button("âš™ï¸ æ‰§è¡Œçºµå‘åˆå¹¶", key="merge_vertical")
				if merge_btn:
					if not common_columns:
						st.error("æ— æ³•åˆå¹¶ï¼šæ²¡æœ‰å…¬å…±åˆ—ã€‚")
					else:
						try:
							merged_parts = []
							for fname, df_part in loaded_dfs.items():
								subset = df_part[list(common_columns)].copy()
								if add_source_col:
									subset["_source_file"] = fname
								merged_parts.append(subset)
							merged_df = pd.concat(merged_parts, ignore_index=True)
							base_name = "merged_common.csv"
							final_name = base_name
							idx = 1
							while final_name in loaded_dfs:
								idx += 1
								final_name = f"merged_common_{idx}.csv"
							try:
								examples_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'examples')
								os.makedirs(examples_dir, exist_ok=True)
								merged_path = os.path.join(examples_dir, final_name)
								merged_df.to_csv(merged_path, index=False, encoding="utf-8")
							except Exception as fs_err:
								st.warning(f"åˆå¹¶æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼Œä½†å†…å­˜ä¾æ—§å¯ç”¨ï¼š{fs_err}")
							loaded_dfs[final_name] = merged_df
							# è®°å½•é¦–é€‰æ–‡ä»¶åï¼Œä¾¿äºä¸Šæ–¹é€‰æ‹©æ¡†è‡ªåŠ¨é€‰ä¸­
							st.session_state["preferred_file_name"] = final_name
							st.session_state["merged_common_df"] = merged_df
							st.success(f"çºµå‘åˆå¹¶æˆåŠŸï¼š{final_name}ï¼Œå½¢çŠ¶ {merged_df.shape}")
							csv_bytes = merged_df.to_csv(index=False).encode('utf-8')
							st.download_button("â¬‡ï¸ ä¸‹è½½ç»“æœ", data=csv_bytes, file_name=final_name, mime="text/csv")
							st.info("åœ¨ä¸Šæ–¹æ–‡ä»¶é€‰æ‹©æ¡†ä¸­å¯é€‰æ‹©è¯¥åˆå¹¶æ–‡ä»¶ç»§ç»­åˆ†æã€‚")
							# åˆå¹¶å®Œæˆåï¼Œé»˜è®¤å°†è¯¥ç»“æœç”¨äºè®­ç»ƒ
							st.session_state["train_df"] = merged_df
							st.session_state["train_source_name"] = final_name
							st.success("è®­ç»ƒå°†é»˜è®¤ä½¿ç”¨è¯¥åˆå¹¶ç»“æœã€‚")
						except Exception as merge_err:
							st.error(f"åˆå¹¶å¤±è´¥ï¼š{merge_err}")

			else:  # æ¨ªå‘åŒ¹é…
				st.caption("ä½¿ç”¨å…¬å…±åˆ—ä½œä¸ºé”®åšå¤šè¡¨ joinï¼Œä¿ç•™æ¯ä¸ªæ–‡ä»¶çš„å…¶ä½™åˆ—ã€‚")
				if not common_columns:
					st.error("æ— æ³•è¿›è¡Œæ¨ªå‘åŒ¹é…ï¼šæ²¡æœ‰å…¬å…±åˆ—ã€‚")
				else:
					key_cols = sorted(common_columns)
					st.info(f"é”®åˆ—ï¼š{', '.join(key_cols)}")
					join_type = st.selectbox("Join ç±»å‹", ["outer", "inner", "left"], index=0, help="outer=ä¿ç•™æ‰€æœ‰é”®; inner=ä»…å…¬å…±é”®; left=ä»¥ç¬¬ä¸€ä¸ªæ–‡ä»¶ä¸ºä¸»è¡¨")
					prefix_cols = st.checkbox("ä¸ºéé”®åˆ—åŠ æ–‡ä»¶åå‰ç¼€ä»¥é˜²å†²çª", value=True, key="prefix_non_key")
					drop_dup = st.checkbox("å¦‚æœæŸæ–‡ä»¶é”®åˆ—æœ‰é‡å¤è¡Œï¼Œä»…ä¿ç•™ç¬¬ä¸€æ¡", value=True, key="drop_dup_keys")
					btn_hmerge = st.button("âš™ï¸ æ‰§è¡Œæ¨ªå‘åŒ¹é…åˆå¹¶", key="merge_horizontal")
					if btn_hmerge:
						try:
							merged_df = None
							for idx_file, (fname, df_part) in enumerate(loaded_dfs.items()):
								work_df = df_part.copy()
								missing_keys = [k for k in key_cols if k not in work_df.columns]
								if missing_keys:
									st.error(f"æ–‡ä»¶ {fname} ç¼ºå¤±é”®åˆ— {missing_keys}ï¼Œè·³è¿‡ã€‚")
									continue
								# å¤„ç†é‡å¤é”®
								if drop_dup and work_df.duplicated(subset=key_cols).any():
									dup_count = work_df.duplicated(subset=key_cols).sum()
									st.warning(f"{fname} é”®åˆ—å­˜åœ¨ {dup_count} ä¸ªé‡å¤ï¼Œå°†ä¿ç•™ç¬¬ä¸€æ¡ã€‚")
									work_df = work_df.drop_duplicates(subset=key_cols, keep='first')
								non_key_cols = [c for c in work_df.columns if c not in key_cols]
								if prefix_cols:
									base_prefix = os.path.splitext(os.path.basename(fname))[0]
									rename_map = {c: f"{base_prefix}__{c}" for c in non_key_cols}
									work_df = work_df.rename(columns=rename_map)
								cols_to_use = key_cols + [c for c in work_df.columns if c not in key_cols]
								if merged_df is None:
									merged_df = work_df[cols_to_use]
								else:
									merged_df = pd.merge(merged_df, work_df[cols_to_use], on=key_cols, how=join_type)
							if merged_df is None:
								st.error("æœªèƒ½ç”Ÿæˆåˆå¹¶ç»“æœï¼ˆå¯èƒ½æ‰€æœ‰æ–‡ä»¶éƒ½è¢«è·³è¿‡ï¼‰")
							else:
								base_name = "merged_horizontal.csv"
								final_name = base_name
								ix = 1
								while final_name in loaded_dfs:
									ix += 1
									final_name = f"merged_horizontal_{ix}.csv"
								try:
									examples_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'examples')
									os.makedirs(examples_dir, exist_ok=True)
									merged_path = os.path.join(examples_dir, final_name)
									merged_df.to_csv(merged_path, index=False, encoding='utf-8')
								except Exception as fs_err:
									st.warning(f"åˆå¹¶æ–‡ä»¶ä¿å­˜å¤±è´¥ï¼Œä½†å†…å­˜ä»å¯ä½¿ç”¨ï¼š{fs_err}")
								loaded_dfs[final_name] = merged_df
								# è®°å½•é¦–é€‰æ–‡ä»¶åï¼Œä¾¿äºä¸Šæ–¹é€‰æ‹©æ¡†è‡ªåŠ¨é€‰ä¸­
								st.session_state["preferred_file_name"] = final_name
								st.success(f"æ¨ªå‘åŒ¹é…åˆå¹¶æˆåŠŸï¼š{final_name}ï¼Œå½¢çŠ¶ {merged_df.shape}")
								csv_bytes = merged_df.to_csv(index=False).encode('utf-8')
								st.download_button("â¬‡ï¸ ä¸‹è½½ç»“æœ", data=csv_bytes, file_name=final_name, mime="text/csv")
								st.info("åœ¨ä¸Šæ–¹æ–‡ä»¶é€‰æ‹©æ¡†ä¸­å¯é€‰æ‹©è¯¥æ¨ªå‘åˆå¹¶æ–‡ä»¶ç»§ç»­åˆ†æã€‚")
								# åˆå¹¶å®Œæˆåï¼Œé»˜è®¤å°†è¯¥ç»“æœç”¨äºè®­ç»ƒ
								st.session_state["train_df"] = merged_df
								st.session_state["train_source_name"] = final_name
								st.success("è®­ç»ƒå°†é»˜è®¤ä½¿ç”¨è¯¥åˆå¹¶ç»“æœã€‚")
						except Exception as e:
							st.error(f"æ¨ªå‘åˆå¹¶å¤±è´¥ï¼š{e}")
	else:
		st.info("ä¸Šä¼  2 ä¸ªåŠä»¥ä¸Šæ–‡ä»¶åï¼Œå°†åœ¨æ­¤æ˜¾ç¤ºå®ƒä»¬çš„å…¬å…±åˆ—ã€‚")

	# åœ¨é¡¶éƒ¨å®¹å™¨ä¸­æ¸²æŸ“é€‰æ‹©æ¡†ï¼Œä½¿å…¶æ˜¾ç¤ºåœ¨åˆå¹¶åŠŸèƒ½ä¸Šæ–¹
	with select_container:
		file_names = list(loaded_dfs.keys())
		preferred = st.session_state.get("preferred_file_name")
		if preferred in file_names:
			default_idx = file_names.index(preferred)
		else:
			default_idx = 0 if file_names else 0
		pick_name = st.selectbox("é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶è¿›è¡Œé¢„è§ˆä¸åˆ†æ", file_names, index=default_idx, key="file_picker_top")
		active_df = loaded_dfs.get(pick_name)
		df_source_name = pick_name

if active_df is not None:
	df = active_df  # ä¿æŒåç»­ä»£ç å˜é‡åä¸å˜

	# å°†â€œå½“å‰æ´»åŠ¨æ•°æ®é›† + æ•°æ®é¢„è§ˆ + æ•°æ®æ¦‚è§ˆâ€ä¸Šç§»åˆ°åˆå¹¶å·¥å…·ä¸Šæ–¹çš„å®¹å™¨ä¸­
	with preview_container:
		st.info(f"å½“å‰æ´»åŠ¨æ•°æ®é›†: {df_source_name}; å½¢çŠ¶: {df.shape}")
		st.write("æ•°æ®é¢„è§ˆï¼š")
		st.dataframe(df.head())

		#ï¼ˆå·²ç§»é™¤å¤šæ–‡ä»¶ä¸»åˆ—åŒ¹é…åŠŸèƒ½ï¼‰
		with st.expander("ğŸ” æ•°æ®æ¦‚è§ˆ", expanded=False):
			st.write("æ•°æ®æè¿°ï¼š")
			st.write(df.describe(include='all').transpose())
			st.write("ç¼ºå¤±å€¼ç»Ÿè®¡ï¼š")
			st.write(df.isnull().sum())

	# ----------- åˆ¤å®šæŒ‰é’®ä¸ç»“æœå±•ç¤ºåŒºå— -------------
	# ä¼˜å…ˆä½¿ç”¨æœ€è¿‘ä¸€æ¬¡åˆå¹¶ç»“æœä½œä¸º LLM åˆ†ææ•°æ®æº
	analysis_df = st.session_state.get("train_df", df)
	analysis_df_name = st.session_state.get("train_source_name", df_source_name)
	st.caption(f"AI åˆ†ææ•°æ®æºï¼š{analysis_df_name}")

	# æ„å»ºç®€æ˜“ profileï¼›åç»­å¯æ›¿æ¢ä¸º ingest.profile
	prof = {
		"columns": [
			{"name": c, "dtype": str(analysis_df[c].dtype), "missing": int(analysis_df[c].isnull().sum()), "unique": int(analysis_df[c].nunique())}
			for c in analysis_df.columns
		]
	}															

	user_question = st.text_area("ä½ çš„é—®é¢˜ï¼ˆå¯é€‰ï¼‰", placeholder="ä¾‹å¦‚ï¼šæˆ‘ä»¬èƒ½å¦é¢„æµ‹ä¹˜å®¢æ˜¯å¦ç”Ÿè¿˜ï¼Ÿæˆ– é¢„æµ‹ä»·æ ¼/åˆ†ç¾¤ç­‰ã€‚")

	col1, col2, col3 = st.columns([1,1,1])
	with col1:
		if st.button("ğŸ” å‘ç°ç ”ç©¶é—®é¢˜"):
			with st.spinner("AI æ­£åœ¨åˆ†ææ•°æ®ï¼Œå¯»æ‰¾æœ‰ä»·å€¼çš„ç ”ç©¶é—®é¢˜ä¸æ¸…æ´—å»ºè®®..."):
				from llm_agent import suggest_research_questions, suggest_cleaning_suggestions
				research_suggestions = suggest_research_questions(prof)
				# åŒæ­¥ç”Ÿæˆ æ¸…æ´—å»ºè®®
				clean_suggest = suggest_cleaning_suggestions(prof, user_question or "")
			st.session_state["research_suggestions"] = research_suggestions
			st.session_state["cleaning_suggest"] = clean_suggest
			st.success("é—®é¢˜å‘ç°å®Œæˆï¼")
	
	with col2:
		if st.button("ğŸ¤– æ™ºèƒ½åˆ¤å®šä»»åŠ¡"):
			with st.spinner("è°ƒç”¨ OpenAI åˆ¤å®šä»»åŠ¡ç±»å‹ï¼Œå¹¶ç”Ÿæˆæ¸…æ´—å»ºè®®..."):
				from llm_agent import suggest_cleaning_suggestions
				plan = detect_task(prof, user_question or "")
				# åŒæ­¥ç”Ÿæˆ æ¸…æ´—å»ºè®®
				clean_suggest = suggest_cleaning_suggestions(prof, user_question or "")
			st.session_state["plan"] = plan
			st.session_state["cleaning_suggest"] = clean_suggest
			st.success("ä»»åŠ¡åˆ¤å®šå®Œæˆï¼")
	
	with col3:
		if "plan" in st.session_state:
			st.download_button("ğŸ“„ ä¸‹è½½åˆ¤å®šç»“æœ", data=json.dumps(st.session_state["plan"], ensure_ascii=False, indent=2),
							   file_name="task_plan.json", mime="application/json")

	# æ–°å¢ï¼šç›®æ ‡åˆ—ä¸ç‰¹å¾ä¿ç•™/åˆ é™¤å»ºè®®
	col_a, col_b = st.columns([1,2])
	with col_a:
		if st.button("ğŸ¯ ç›®æ ‡ä¸ç‰¹å¾å»ºè®®"):
			with st.spinner("AI æ­£åœ¨åˆ†æç›®æ ‡åˆ—ä¸åº”ä¿ç•™/åˆ é™¤çš„åˆ—..."):
				from llm_agent import suggest_target_and_features
				feat_suggest = suggest_target_and_features(prof, user_question or "")
			st.session_state["feature_suggest"] = feat_suggest
			st.success("åˆ—å»ºè®®å·²ç”Ÿæˆï¼")
	with col_b:
		if st.session_state.get("feature_suggest"):
			st.caption("ä½ å¯ä»¥å°†å»ºè®®ç›´æ¥åº”ç”¨åˆ°åç»­è®­ç»ƒçš„æ•°æ®åˆ—ä¸­ã€‚")

	# æ˜¾ç¤ºç ”ç©¶é—®é¢˜å»ºè®®
	if "research_suggestions" in st.session_state:
		st.subheader("ğŸ” AI æ•°æ®æ´å¯Ÿï¼šå¯ç ”ç©¶çš„é—®é¢˜")
		
		suggestions = st.session_state["research_suggestions"]
		
		def display_research_suggestions(suggestions):
			"""å¯è¯»åŒ–æ˜¾ç¤ºç ”ç©¶é—®é¢˜å»ºè®®"""
			
			# æ£€æŸ¥ suggestions æ˜¯å¦ä¸ºå­—å…¸
			if not isinstance(suggestions, dict):
				st.error("âŒ ç ”ç©¶å»ºè®®æ•°æ®æ ¼å¼é”™è¯¯")
				st.json(suggestions)
				return
			
			# ç ”ç©¶é—®é¢˜
			questions = suggestions.get("research_questions", [])
			if questions:
				st.markdown("### ğŸ’¡ **æ¨èç ”ç©¶é—®é¢˜**")
				
				for i, q in enumerate(questions):
					with st.expander(f"ğŸ“‹ é—®é¢˜ {i+1}: {q.get('question', 'æœªçŸ¥é—®é¢˜')}", expanded=i==0):
						col1, col2 = st.columns(2)
						with col1:
							st.markdown(f"**ç±»å‹**: {q.get('type', 'æœªçŸ¥')}")
							st.markdown(f"**éš¾åº¦**: {q.get('difficulty', 'æœªçŸ¥')}")
						with col2:
							if q.get('target_column'):
								st.markdown(f"**ç›®æ ‡åˆ—**: `{q.get('target_column')}`")
							methods = q.get('required_methods', [])
							if methods:
								st.markdown(f"**æ¨èæ–¹æ³•**: {', '.join(methods)}")
						
						st.markdown("**å•†ä¸šä»·å€¼**:")
						st.info(q.get('business_value', 'æœªæä¾›'))
			
			# åº”ç”¨åœºæ™¯
			scenarios = suggestions.get("application_scenarios", [])
			if scenarios:
				st.markdown("### ğŸ¯ **åº”ç”¨åœºæ™¯**")
				for i, scenario in enumerate(scenarios):
					st.markdown(f"**{i+1}.** {scenario}")
			
			# å…³é”®æ´å¯Ÿæ½œåŠ›
			insights = suggestions.get("key_insights_potential", [])
			if insights:
				st.markdown("### ğŸ”® **å¯èƒ½å‘ç°çš„æ´å¯Ÿ**")
				for insight in insights:
					st.markdown(f"â€¢ {insight}")
			
			# æ•°æ®é›†ä¼˜åŠ¿
			strengths = suggestions.get("dataset_strengths", [])
			if strengths:
				st.markdown("### âœ¨ **æ•°æ®é›†ä¼˜åŠ¿**")
				for strength in strengths:
					st.markdown(f"âœ… {strength}")
			
			# é™åˆ¶å’Œæ³¨æ„äº‹é¡¹
			limitations = suggestions.get("limitations", [])
			if limitations:
				st.markdown("### âš ï¸ **æ³¨æ„äº‹é¡¹**")
				for limitation in limitations:
					st.warning(f"âš ï¸ {limitation}")

			# å»ºè®®
			recommendations = suggestions.get("recommendations", {})
			if recommendations:
				st.markdown("### ğŸ¯ **è¡ŒåŠ¨å»ºè®®**")
				# æ£€æŸ¥ recommendations æ˜¯å¦ä¸ºå­—å…¸
				if isinstance(recommendations, dict):
					priority = recommendations.get("priority_questions", [])
					if priority:
						st.markdown("**ğŸ”¥ ä¼˜å…ˆç ”ç©¶é—®é¢˜**:")
						for p in priority:
							st.markdown(f"â€¢ {p}")
					
					next_steps = recommendations.get("next_steps", [])
					if next_steps:
						st.markdown("**ğŸ“‹ å»ºè®®æ­¥éª¤**:")
						for step in next_steps:
							st.markdown(f"â€¢ {step}")
					
					additional_data = recommendations.get("additional_data", [])
					if additional_data:
						st.markdown("**ğŸ“Š å¯èƒ½éœ€è¦çš„é¢å¤–æ•°æ®**:")
						for data in additional_data:
							st.markdown(f"â€¢ {data}")
				else:
					# å¦‚æœ recommendations æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥æ˜¾ç¤º
					st.markdown(f"**ğŸ’¡ å»ºè®®**: {recommendations}")
		
		display_research_suggestions(suggestions)
		
		# åŸå§‹ JSON (å¯é€‰)
		with st.expander("ğŸ” æŸ¥çœ‹è¯¦ç»†åˆ†æç»“æœ (JSON)", expanded=False):
			st.json(suggestions)
		
		st.success("ğŸ’¡ åŸºäºä»¥ä¸Šåˆ†æï¼Œä½ å¯ä»¥é€‰æ‹©æ„Ÿå…´è¶£çš„é—®é¢˜è¿›è¡Œæ·±å…¥ç ”ç©¶ï¼")

	if "plan" in st.session_state:
		st.subheader("ğŸ¤– AI æ™ºèƒ½åˆ¤å®šç»“æœ")
		
		plan = st.session_state["plan"]
		
		# å¯è¯»åŒ–æ˜¾ç¤ºåˆ¤å®šç»“æœ
		def display_readable_plan(plan):
			# ä»»åŠ¡ç±»å‹
			task_type = plan.get("task_type", "æœªçŸ¥")
			task_type_cn = {"classification": "åˆ†ç±»ä»»åŠ¡", "regression": "å›å½’ä»»åŠ¡", "clustering": "èšç±»ä»»åŠ¡"}.get(task_type, task_type)
			
			st.markdown(f"### ğŸ“Š **ä»»åŠ¡ç±»å‹**: {task_type_cn}")
			if task_type == "classification":
				st.info("ğŸ¯ è¿™æ˜¯ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯é¢„æµ‹ç¦»æ•£çš„ç±»åˆ«æ ‡ç­¾")
			elif task_type == "regression":
				st.info("ğŸ“ˆ è¿™æ˜¯ä¸€ä¸ªå›å½’ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯é¢„æµ‹è¿ç»­çš„æ•°å€¼")
			elif task_type == "clustering":
				st.info("ğŸ” è¿™æ˜¯ä¸€ä¸ªèšç±»ä»»åŠ¡ï¼Œç›®æ ‡æ˜¯å‘ç°æ•°æ®ä¸­çš„éšè—æ¨¡å¼")
			
			# ç›®æ ‡å€™é€‰åˆ—
			targets = plan.get("target_candidates", [])
			if targets:
				st.markdown("### ğŸ¯ **æ¨èç›®æ ‡åˆ—**")
				for i, target in enumerate(targets):
					st.markdown(f"**{i+1}.** `{target}`")
			else:
				st.warning("âš ï¸ æœªæ‰¾åˆ°æ˜ç¡®çš„ç›®æ ‡åˆ—ï¼Œè¯·æ‰‹åŠ¨é€‰æ‹©")
			
			# æ¨èç®—æ³•
			algorithms = plan.get("algorithms", [])
			if algorithms:
				st.markdown("### ğŸ¤– **æ¨èç®—æ³•**")
				algo_names = {
					"xgboost": "XGBoost (æç«¯æ¢¯åº¦æå‡)",
					"ridge": "Ridge å›å½’ (å²­å›å½’)",
					"knn": "K-è¿‘é‚»ç®—æ³•",
					"random_forest": "éšæœºæ£®æ—",
					"linear_regression": "çº¿æ€§å›å½’",
					"logistic_regression": "é€»è¾‘å›å½’",
					"svm": "æ”¯æŒå‘é‡æœº",
					"mlp": "å¤šå±‚æ„ŸçŸ¥æœº"
				}
				
				cols = st.columns(min(len(algorithms), 3))
				for i, algo in enumerate(algorithms):
					with cols[i % 3]:
						algo_display = algo_names.get(algo, algo.replace('_', ' ').title())
						st.markdown(f"**{i+1}.** {algo_display}")
			
			# è¯„ä¼°æŒ‡æ ‡
			metrics = plan.get("metrics", [])
			if metrics:
				st.markdown("### ğŸ“ **è¯„ä¼°æŒ‡æ ‡**")
				metric_names = {
					"rmse": "RMSE (å‡æ–¹æ ¹è¯¯å·®)",
					"mae": "MAE (å¹³å‡ç»å¯¹è¯¯å·®)", 
					"r2": "RÂ² (å†³å®šç³»æ•°)",
					"accuracy": "å‡†ç¡®ç‡",
					"f1": "F1 åˆ†æ•°",
					"precision": "ç²¾ç¡®ç‡",
					"recall": "å¬å›ç‡",
					"auc": "AUC (æ›²çº¿ä¸‹é¢ç§¯)"
				}
				
				metric_cols = st.columns(min(len(metrics), 3))
				for i, metric in enumerate(metrics):
					with metric_cols[i % 3]:
						metric_display = metric_names.get(metric, metric.upper())
						st.markdown(f"**â€¢** {metric_display}")
			
			# äº¤å‰éªŒè¯è®¾ç½®
			cv_info = plan.get("cv", {})
			if cv_info:
				st.markdown("### âœ… **äº¤å‰éªŒè¯è®¾ç½®**")
				folds = cv_info.get("folds", 5)
				stratified = cv_info.get("stratified", False)
				
				col1, col2 = st.columns(2)
				with col1:
					st.metric("äº¤å‰éªŒè¯æŠ˜æ•°", f"{folds} æŠ˜")
				with col2:
					stratify_text = "æ˜¯" if stratified else "å¦"
					st.metric("åˆ†å±‚é‡‡æ ·", stratify_text)
			
			# ç±»åˆ«ä¸å¹³è¡¡ä¿¡æ¯
			imbalance = plan.get("imbalance", {})
			if imbalance and imbalance.get("is_imbalanced"):
				st.markdown("### âš–ï¸ **æ•°æ®ä¸å¹³è¡¡è­¦å‘Š**")
				ratio = imbalance.get("ratio")
				if ratio:
					st.warning(f"æ£€æµ‹åˆ°æ•°æ®ä¸å¹³è¡¡ï¼Œä¸»è¦ç±»åˆ«å æ¯”: {ratio:.1%}")
					st.caption("å»ºè®®è€ƒè™‘ä½¿ç”¨ç±»åˆ«æƒé‡å¹³è¡¡æˆ–é‡‡æ ·æŠ€æœ¯")
		
		# æ˜¾ç¤ºå¯è¯»åŒ–ç»“æœ
		display_readable_plan(plan)
		
		# å¯é€‰æ˜¾ç¤ºåŸå§‹ JSON
		with st.expander("ğŸ” æŸ¥çœ‹è¯¦ç»† JSON ç»“æœ", expanded=False):
			st.json(plan)
		
		st.caption("ğŸ’¡ ä»¥ä¸Šç»“æœå°†è‡ªåŠ¨åº”ç”¨åˆ°è®­ç»ƒè®¾ç½®ä¸­ï¼Œä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨è°ƒæ•´å‚æ•°ã€‚")

	# å·²ç§»é™¤â€œç›®æ ‡åˆ—ä¸ç‰¹å¾é€‰æ‹©å»ºè®®â€åŠŸèƒ½ä¸åº”ç”¨å…¥å£

	# æ˜¾ç¤º æ•°æ®æ¸…æ´—å»ºè®®ï¼ˆåˆå¹¶åˆ°ä»¥ä¸Šä¸¤ä¸ªæµç¨‹åå±•ç¤ºï¼‰
	if st.session_state.get("cleaning_suggest"):
		st.subheader("ğŸ§¹ æ•°æ®æ¸…æ´—å»ºè®®")
		cs = st.session_state["cleaning_suggest"]
		# Drop
		drops = cs.get("drop_columns", [])
		with st.expander(f"ğŸ—‘ï¸ å»ºè®®åˆ é™¤åˆ—ï¼ˆ{len(drops)}ï¼‰", expanded=False):
			if drops:
				for d in drops:
					st.write(f"- {d.get('name')}: {d.get('reason','')}")
			else:
				st.write("æ— ")
		# Imputations
		imps = cs.get("imputations", [])
		with st.expander(f"ğŸ§© ç¼ºå¤±å€¼å¡«å……å»ºè®®ï¼ˆ{len(imps)}ï¼‰", expanded=False):
			if imps:
				for d in imps:
					st.write(f"- {d.get('name')}: {d.get('strategy')}")
			else:
				st.write("æ— ")
		# Type casts and parse dates
		casts = cs.get("type_casts", [])
		pdates = cs.get("parse_dates", [])
		with st.expander(f"ğŸ§­ ç±»å‹è½¬æ¢å»ºè®®ï¼ˆ{len(casts)}ï¼‰/ æ—¥æœŸè§£æï¼ˆ{len(pdates)}ï¼‰", expanded=False):
			if casts:
				for d in casts:
					st.write(f"- {d.get('name')} -> {d.get('to_dtype')}: {d.get('reason','')}")
			else:
				st.write("ç±»å‹è½¬æ¢ï¼šæ— ")
			if pdates:
				st.write("æ—¥æœŸè§£æï¼š")
				st.code("\n".join(pdates))
			else:
				st.write("æ—¥æœŸè§£æï¼šæ— ")
		# Scaling
		scaling = cs.get("scaling", {}) or {}
		with st.expander("ğŸ“ ç¼©æ”¾å»ºè®®", expanded=False):
			st.write(f"å»ºè®®ç¼©æ”¾: {'æ˜¯' if scaling.get('apply') else 'å¦'}")
			sc_cols = scaling.get("columns", [])
			if sc_cols:
				st.code("\n".join(sc_cols))
			else:
				st.write("åˆ—ï¼šæ— ")
		# Outliers
		outliers = cs.get("outliers", {}) or {}
		with st.expander("ğŸ“‰ å¼‚å¸¸å€¼å¤„ç†å»ºè®®", expanded=False):
			st.write(f"å»ºè®®å¤„ç†: {'æ˜¯' if outliers.get('apply') else 'å¦'}; æ–¹æ³•: {outliers.get('method','iqr_clip')}")
			out_cols = outliers.get("columns", [])
			if out_cols:
				st.code("\n".join(out_cols))
			else:
				st.write("åˆ—ï¼šæ— ")
		# Text processing
		txts = cs.get("text_processing", [])
		with st.expander(f"ğŸ“ æ–‡æœ¬å¤„ç†å»ºè®®ï¼ˆ{len(txts)}ï¼‰", expanded=False):
			if txts:
				for d in txts:
					st.write(f"- {d.get('name')}: {d.get('suggestion')}")
			else:
				st.write("æ— ")
		# Leakage
		leaks = cs.get("leakage_risk", [])
		with st.expander(f"âš ï¸ å¯èƒ½çš„æ³„éœ²é£é™©åˆ—ï¼ˆ{len(leaks)}ï¼‰", expanded=False):
			if leaks:
				st.code("\n".join(leaks))
			else:
				st.write("æ— ")
		st.caption(cs.get("notes") or "")

		# ============ ä¸€é”®ä¸æ‰‹åŠ¨æ¸…æ´—æ“ä½œ ============
		st.markdown("---")
		st.markdown("### âš™ï¸ åº”ç”¨æ¸…æ´—æ“ä½œ")

		# å½“å‰ç”¨äºè®­ç»ƒ/åˆ†æçš„æ•°æ®
		work_df = st.session_state.get("train_df", analysis_df)
		work_name = st.session_state.get("train_source_name", analysis_df_name)
		st.caption(f"æ¸…æ´—ç›®æ ‡æ•°æ®é›†ï¼š{work_name} å½¢çŠ¶ï¼š{work_df.shape}")

		# ä¸€é”®åº”ç”¨ GPT å»ºè®®
		def _iqr_clip_inline(df, cols, whisker: float = 1.5):
			import numpy as np
			df = df.copy()
			for c in cols:
				if c in df.columns:
					try:
						s = pd.to_numeric(df[c], errors='coerce')
						q1, q3 = s.quantile(0.25), s.quantile(0.75)
						iqr = q3 - q1
						low, high = q1 - whisker * iqr, q3 + whisker * iqr
						df[c] = s.clip(lower=low, upper=high)
					except Exception:
						pass
			return df

		def _apply_type_casts(df, casts):
			df = df.copy()
			for item in casts or []:
				name = item.get('name')
				to_dtype = (item.get('to_dtype') or '').lower()
				if name not in df.columns:
					continue
				try:
					if to_dtype in ('float','float64','double','number'):
						df[name] = pd.to_numeric(df[name], errors='coerce')
					elif to_dtype in ('int','int64','long'):
						df[name] = pd.to_numeric(df[name], errors='coerce').astype('Int64')
					elif to_dtype in ('bool','boolean'):
						df[name] = df[name].astype('boolean')
					elif to_dtype in ('category','categorical'):
						df[name] = df[name].astype('category')
					elif to_dtype in ('string','str','object'):
						df[name] = df[name].astype('string')
					# else: leave as is
				except Exception:
					pass
			return df

		def _apply_imputations(df, imputations):
			df = df.copy()
			for item in imputations or []:
				name = item.get('name')
				strategy = (item.get('strategy') or 'most_frequent').lower()
				if name not in df.columns:
					continue
				try:
					if strategy == 'median':
						val = pd.to_numeric(df[name], errors='coerce').median()
						df[name] = pd.to_numeric(df[name], errors='coerce').fillna(val)
					elif strategy == 'mean':
						val = pd.to_numeric(df[name], errors='coerce').mean()
						df[name] = pd.to_numeric(df[name], errors='coerce').fillna(val)
					else:  # most_frequent
						val = df[name].mode(dropna=True)
						val = val.iloc[0] if len(val) else None
						if val is not None:
							df[name] = df[name].fillna(val)
				except Exception:
					pass
			return df

		col_btn1, col_btn2 = st.columns([1,2])
		with col_btn1:
			if st.button("âš¡ ä¸€é”®åº”ç”¨ GPT æ¸…æ´—å»ºè®®"):
				try:
					new_df = work_df.copy()
					# Drop
					to_drop = [d.get('name') for d in (cs.get('drop_columns') or []) if d.get('name') in new_df.columns]
					if to_drop:
						new_df = new_df.drop(columns=to_drop, errors='ignore')
					# Parse dates
					for c in (cs.get('parse_dates') or []):
						if c in new_df.columns:
							try:
								new_df[c] = pd.to_datetime(new_df[c], errors='coerce')
							except Exception:
								pass
					# Type casts
					new_df = _apply_type_casts(new_df, cs.get('type_casts'))
					# Imputations
					new_df = _apply_imputations(new_df, cs.get('imputations'))
					# Outliers
					out_cols = []
					out_meta = cs.get('outliers') or {}
					if isinstance(out_meta, dict) and out_meta.get('apply'):
						out_cols = [c for c in (out_meta.get('columns') or []) if c in new_df.columns]
					if out_cols:
						new_df = _iqr_clip_inline(new_df, out_cols)

					st.session_state["train_df"] = new_df
					st.session_state["train_source_name"] = f"{work_name}ï¼ˆå·²æŒ‰GPTå»ºè®®æ¸…æ´—ï¼‰"
					st.success(f"å·²åº”ç”¨ GPT æ¸…æ´—å»ºè®®ï¼Œå½“å‰å½¢çŠ¶ï¼š{new_df.shape}")
				except Exception as e:
					st.error(f"åº”ç”¨å¤±è´¥ï¼š{e}")

		with col_btn2:
			st.caption("æˆ–æ‰‹åŠ¨é€‰æ‹©ä»¥ä¸‹æ¸…æ´—æ“ä½œï¼š")
			# æ‰‹åŠ¨é€‰æ‹©
			all_cols = list(work_df.columns)
			default_drop = [d.get('name') for d in (cs.get('drop_columns') or []) if d.get('name') in all_cols]
			pick_drop = st.multiselect("è¦åˆ é™¤çš„åˆ—", options=all_cols, default=default_drop)

			default_dates = [c for c in (cs.get('parse_dates') or []) if c in all_cols]
			pick_dates = st.multiselect("è¦è§£æä¸ºæ—¥æœŸçš„åˆ—", options=all_cols, default=default_dates)

			out_meta = cs.get('outliers') or {}
			default_out = [c for c in (out_meta.get('columns') or []) if c in all_cols]
			pick_outliers = st.multiselect("IQR è£å‰ªçš„æ•°å€¼åˆ—", options=all_cols, default=default_out)

			apply_imp = st.checkbox("æŒ‰å»ºè®®å¡«å……ç¼ºå¤±å€¼ï¼ˆæ•°å€¼: ä¸­ä½/å‡å€¼ï¼›ç±»åˆ«: ä¼—æ•°ï¼‰", value=True)
			apply_casts = st.checkbox("æŒ‰å»ºè®®è¿›è¡Œç±»å‹è½¬æ¢", value=True)

			if st.button("ğŸ› ï¸ åº”ç”¨é€‰ä¸­æ¸…æ´—æ“ä½œ"):
				try:
					new_df = work_df.copy()
					if pick_drop:
						new_df = new_df.drop(columns=pick_drop, errors='ignore')
					for c in pick_dates:
						if c in new_df.columns:
							try:
								new_df[c] = pd.to_datetime(new_df[c], errors='coerce')
							except Exception:
								pass
					if apply_casts:
						new_df = _apply_type_casts(new_df, cs.get('type_casts'))
					if apply_imp:
						new_df = _apply_imputations(new_df, cs.get('imputations'))
					if pick_outliers:
						new_df = _iqr_clip_inline(new_df, pick_outliers)
					st.session_state["train_df"] = new_df
					st.session_state["train_source_name"] = f"{work_name}ï¼ˆå·²æŒ‰æ‰‹åŠ¨æ¸…æ´—ï¼‰"
					st.success(f"å·²åº”ç”¨æ‰‹åŠ¨æ¸…æ´—ï¼Œå½“å‰å½¢çŠ¶ï¼š{new_df.shape}")
				except Exception as e:
					st.error(f"åº”ç”¨å¤±è´¥ï¼š{e}")

		# ============ æ¸…æ´—åæ•°æ®å¯è§†åŒ– ============
		st.markdown("---")
		st.markdown("### ğŸ“Š æ•°æ®å¯è§†åŒ–ï¼ˆæ¸…æ´—åï¼‰")
		viz_df = st.session_state.get("train_df", work_df)
		viz_name = st.session_state.get("train_source_name", work_name)
		st.caption(f"åŸºäºæ¸…æ´—åçš„æ•°æ®ï¼š{viz_name} ï¼›å½¢çŠ¶ï¼š{viz_df.shape}")

		enable_viz = st.checkbox("å¯ç”¨å¯è§†åŒ–", value=True, key="enable_viz_after_clean")
		if enable_viz and viz_df is not None and len(viz_df.columns) > 0:
			try:
				import altair as alt
				_has_altair = True
			except Exception:
				alt = None
				_has_altair = False
			col_left, col_right = st.columns([1,2])
			with col_left:
				picked_col = st.selectbox("é€‰æ‹©è¦å¯è§†åŒ–çš„åˆ—", options=list(viz_df.columns), key="viz_col_select")
				if picked_col is not None:
					series = viz_df[picked_col]
					is_num = pd.api.types.is_numeric_dtype(series)
					is_dt = pd.api.types.is_datetime64_any_dtype(series)
					if is_num:
						bins = st.slider("ç›´æ–¹å›¾åˆ†ç®±æ•°", min_value=5, max_value=100, value=30, step=5, key="viz_bins")
					elif is_dt:
						freq = st.selectbox("æ—¶é—´èšåˆç²’åº¦", ["D","W","M"], index=0, help="æŒ‰å¤©/å‘¨/æœˆç»Ÿè®¡è®¡æ•°", key="viz_dt_freq")
					else:
						topk = st.slider("ç±»åˆ«Top N", min_value=5, max_value=100, value=20, step=5, key="viz_topk")

			with col_right:
				if 'picked_col' in locals() and picked_col is not None:
					series = viz_df[picked_col]
					is_num = pd.api.types.is_numeric_dtype(series)
					is_dt = pd.api.types.is_datetime64_any_dtype(series)
					if is_num:
						# æ•°å€¼ï¼šç›´æ–¹å›¾ + ç®±çº¿å›¾ï¼ˆæ—  Altair æ—¶é€€åŒ–ä¸ºæŸ±çŠ¶å›¾ï¼‰
						base_df = pd.DataFrame({picked_col: pd.to_numeric(series, errors='coerce')})
						if _has_altair:
							hist = alt.Chart(base_df).mark_bar().encode(
								alt.X(f"{picked_col}:Q", bin=alt.Bin(maxbins=bins)),
								y='count()'
							).properties(height=220)
							box = alt.Chart(base_df).mark_boxplot().encode(x=alt.X(f"{picked_col}:Q")).properties(height=120)
							st.altair_chart(hist & box, use_container_width=True)
						else:
							# è®¡ç®—ç›´æ–¹å›¾æ•°æ®å¹¶ç”¨åŸç”Ÿ bar_chart å±•ç¤º
							try:
								import numpy as np
								counts, bin_edges = np.histogram(base_df[picked_col].dropna(), bins=bins)
								centers = (bin_edges[:-1] + bin_edges[1:]) / 2
								df_hist = pd.DataFrame({"bin": centers, "count": counts})
								st.bar_chart(df_hist.set_index("bin")["count"])
							except Exception:
								st.line_chart(base_df[picked_col])
					elif is_dt:
						# æ—¶é—´ï¼šæŒ‰ç²’åº¦è®¡æ•°
						df_dt = pd.DataFrame({picked_col: pd.to_datetime(series, errors='coerce')}).dropna()
						if not df_dt.empty:
							df_dt['__bucket__'] = df_dt[picked_col].dt.to_period(freq).dt.start_time
							cnt = df_dt.groupby('__bucket__').size().reset_index(name='count')
							if _has_altair:
								chart = alt.Chart(cnt).mark_bar().encode(x='__bucket__:T', y='count:Q').properties(height=260)
								st.altair_chart(chart, use_container_width=True)
							else:
								cnt = cnt.set_index('__bucket__')
								st.bar_chart(cnt['count'])
						else:
							st.info("æ‰€é€‰åˆ—æ— æ³•è§£æä¸ºæœ‰æ•ˆæ—¶é—´æ ¼å¼ã€‚")
					else:
						# ç±»åˆ«ï¼šTopK é¢‘æ¬¡æ¡å½¢å›¾
						vc = series.astype('string').value_counts().reset_index()
						vc.columns = [picked_col, 'count']
						vc = vc.head(topk)
						if _has_altair:
							chart = alt.Chart(vc).mark_bar().encode(
								y=alt.Y(f"{picked_col}:N", sort='-x'),
								x=alt.X('count:Q')
							).properties(height=max(200, 16*len(vc)))
							st.altair_chart(chart, use_container_width=True)
						else:
							st.bar_chart(vc.set_index(picked_col)['count'])
		else:
			st.info("æ— å¯è§†åŒ–æ•°æ®å¯ç”¨æˆ–æœªé€‰æ‹©åˆ—ã€‚")

	# ------------------ è®­ç»ƒè®¾ç½®ä¸è®­ç»ƒæµç¨‹ï¼ˆæœ€å°æ¥å…¥ï¼‰ ------------------
	import sys, os
	sys.path.append(os.path.dirname(os.path.dirname(__file__)))
	from core import cleandata, train as train_core

	st.subheader("ğŸ› ï¸ è®­ç»ƒè®¾ç½®ï¼ˆæœ¬åœ°ï¼‰")

	# è®­ç»ƒæ•°æ®æ¥æºé€‰æ‹©ï¼šé»˜è®¤ä½¿ç”¨æœ€æ–°åˆå¹¶ç»“æœï¼ˆè‹¥å­˜åœ¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨å½“å‰æ´»åŠ¨æ•°æ®é›†
	options = []
	if "train_df" in st.session_state and "train_source_name" in st.session_state:
		options.append(f"æœ€æ–°åˆå¹¶ç»“æœï¼ˆ{st.session_state['train_source_name']}ï¼‰")
	options.append(f"å½“å‰æ´»åŠ¨æ•°æ®é›†ï¼ˆ{df_source_name}ï¼‰")
	default_idx = 0 if options and options[0].startswith("æœ€æ–°åˆå¹¶ç»“æœ") else 0
	selected_source = st.radio("è®­ç»ƒæ•°æ®æ¥æº", options, index=default_idx, horizontal=True)
	if selected_source.startswith("æœ€æ–°åˆå¹¶ç»“æœ") and "train_df" in st.session_state:
		train_df = st.session_state["train_df"]
		train_source_name = st.session_state.get("train_source_name", "æœ€æ–°åˆå¹¶ç»“æœ")
	else:
		train_df = df
		train_source_name = df_source_name
	st.info(f"è®­ç»ƒæ•°æ®é›†ï¼š{train_source_name}ï¼›å½¢çŠ¶ï¼š{train_df.shape}")
	
	# æ™ºèƒ½åº”ç”¨ AI åˆ¤å®šç»“æœ
	plan = st.session_state.get("plan", {})
	ai_targets = plan.get("target_candidates", [])
	ai_task_type = plan.get("task_type", "")
	ai_algorithms = plan.get("algorithms", [])
	ai_cv = plan.get("cv", {})
	
	# ç›®æ ‡åˆ—é€‰æ‹© - ä¼˜å…ˆä½¿ç”¨ AI æ¨è
	available_columns = [c for c in train_df.columns if c != ""]
	if ai_targets and ai_targets[0] in available_columns:
		default_target_index = available_columns.index(ai_targets[0])
		st.success(f"ğŸ¤– AI æ¨èç›®æ ‡åˆ—: {ai_targets[0]}")
	else:
		default_target_index = 0
	
	target = st.selectbox("ç›®æ ‡åˆ—", options=available_columns, index=default_target_index)
	
	# è‡ªåŠ¨æ¨èä»»åŠ¡ç±»å‹
	if target:
		target_series = train_df[target]
		target_series = target_series.dropna()  # å»é™¤ç¼ºå¤±å€¼
		
		# æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å€¼ç±»å‹
		is_numeric = pd.api.types.is_numeric_dtype(target_series)
		unique_count = target_series.nunique()
		total_count = len(target_series)
		
		# æ¨èé€»è¾‘
		if is_numeric and unique_count > 20 and unique_count / total_count > 0.05:
			recommended_task = "regression"
			reason = f"æ•°å€¼ç±»å‹ï¼Œ{unique_count} ä¸ªå”¯ä¸€å€¼"
		else:
			recommended_task = "classification" 
			if not is_numeric:
				reason = f"éæ•°å€¼ç±»å‹ï¼Œ{unique_count} ä¸ªç±»åˆ«"
			else:
				reason = f"æ•°å€¼ç±»å‹ä½†åªæœ‰ {unique_count} ä¸ªå”¯ä¸€å€¼ï¼Œå¯èƒ½æ˜¯åˆ†ç±»"
		
		st.info(f"ğŸ¤– æ¨èä»»åŠ¡ç±»å‹: **{recommended_task}** ({reason})")
		
		# æ˜¾ç¤ºç›®æ ‡å˜é‡çš„åŸºæœ¬ç»Ÿè®¡
		col1, col2 = st.columns(2)
		with col1:
			st.metric("å”¯ä¸€å€¼æ•°é‡", unique_count)
		with col2:
			st.metric("æ ·æœ¬æ•°é‡", total_count)
		
		if unique_count <= 10:
			st.write("ç›®æ ‡å˜é‡çš„å€¼åˆ†å¸ƒ:")
			value_counts = target_series.value_counts().head(10)
			st.bar_chart(value_counts)
	
	# ä»»åŠ¡ç±»å‹é€‰æ‹© - æ™ºèƒ½åº”ç”¨ AI æ¨è
	task_options = ["classification", "regression"]
	if ai_task_type and ai_task_type in task_options:
		default_task_index = task_options.index(ai_task_type)
		st.success(f"ğŸ¤– AI æ¨èä»»åŠ¡ç±»å‹: {ai_task_type}")
	else:
		default_task_index = 0
	
	task_type = st.selectbox("ä»»åŠ¡ç±»å‹", task_options, index=default_task_index)
	
	# ç®—æ³•é€‰æ‹© - æ™ºèƒ½åº”ç”¨ AI æ¨è
	available_algos = ["logreg","rf","xgb","knn","mlp"] if task_type=="classification" else ["linreg","ridge","rf","xgb","knn","mlp"]
	
	# æ˜ å°„ AI æ¨èçš„ç®—æ³•åç§°åˆ°æœ¬åœ°ç®—æ³•åç§°
	algo_mapping = {
		"xgboost": "xgb",
		"random_forest": "rf", 
		"ridge": "ridge",
		"knn": "knn",
		"logistic_regression": "logreg",
		"linear_regression": "linreg",
		"mlp": "mlp"
	}
	
	ai_algos_mapped = []
	if ai_algorithms:
		for ai_algo in ai_algorithms:
			mapped_algo = algo_mapping.get(ai_algo, ai_algo)
			if mapped_algo in available_algos:
				ai_algos_mapped.append(mapped_algo)
		
		if ai_algos_mapped:
			st.success(f"ğŸ¤– AI æ¨èç®—æ³•: {', '.join(ai_algos_mapped)}")
			default_algos = ai_algos_mapped
		else:
			default_algos = ["rf","xgb"]
	else:
		default_algos = ["rf","xgb"]
	
	picked = st.multiselect(
		"å€™é€‰ç®—æ³•",
		available_algos,
		default=default_algos
	)
	budget = st.slider("æ¯æ¨¡å‹æœç´¢æ¬¡æ•° (n_iter)", 10, 80, 30)
	
	# äº¤å‰éªŒè¯æŠ˜æ•° - æ™ºèƒ½åº”ç”¨ AI æ¨è
	default_folds = ai_cv.get("folds", 5) if ai_cv else 5
	if ai_cv and "folds" in ai_cv:
		st.success(f"ğŸ¤– AI æ¨è CV æŠ˜æ•°: {default_folds}")
	
	folds = st.slider("CV æŠ˜æ•°", 3, 10, default_folds)


	# è¯„ä¼°è¡Œæ•°é™åˆ¶è®¾ç½®
	with st.expander("âš™ï¸ è¯„ä¼°æ•°æ®é‡è®¾ç½®", expanded=False):
		col_a, col_b = st.columns([1,2])
		with col_a:
			use_eval_limit = st.checkbox("é™åˆ¶è¯„ä¼°è¡Œæ•°", value=True, help="ä»…åœ¨è¯„ä¼°æŒ‡æ ‡/é¢„æµ‹æ—¶ä½¿ç”¨æµ‹è¯•é›†å‰ N è¡Œï¼Œé€‚åˆå¿«é€Ÿè¿­ä»£ã€‚")
		with col_b:
			if use_eval_limit:
				custom_eval_rows = st.number_input("è¯„ä¼°æœ€å¤§è¡Œæ•° N", min_value=50, max_value=20000, value=500, step=50, help="è¶…è¿‡è¯¥è¡Œæ•°æ—¶ä»…æˆªå–å‰ N è¡Œï¼›ä¸å½±å“æ¨¡å‹è®­ç»ƒã€‚")
			else:
				custom_eval_rows = None

	if st.button("å¼€å§‹è®­ç»ƒ"):
		X_train, X_test, y_train, y_test, pre, col_info = cleandata.prepare(train_df, target, task_type)
		leaderboard, artifacts = train_core.run_all(
			X_train, y_train, X_test, y_test,
			task_type=task_type,
			picked_models=picked,
			preprocessor=pre,
			n_iter=budget,
			cv_folds=folds,
			artifacts_dir="artifacts"
		)
		st.success("è®­ç»ƒå®Œæˆï¼")
		st.dataframe(leaderboard)
		# æ ¹æ®ç”¨æˆ·è®¾ç½®é™åˆ¶è¯„ä¼°é˜¶æ®µä½¿ç”¨çš„æµ‹è¯•é›†è¡Œæ•°
		if custom_eval_rows and custom_eval_rows > 0 and len(X_test) > custom_eval_rows:
			n_eval = int(min(custom_eval_rows, len(X_test)))
			if hasattr(X_test, 'head'):
				try:
					X_test_eval = X_test.head(n_eval)
				except Exception:
					X_test_eval = X_test[:n_eval]
			else:
				X_test_eval = X_test[:n_eval]
			if hasattr(y_test, 'iloc'):
				y_test_eval = y_test.iloc[:n_eval]
			else:
				y_test_eval = y_test[:n_eval]
			st.info(f"è¯„ä¼°è¡Œæ•°é™åˆ¶å¯ç”¨ï¼šä½¿ç”¨æµ‹è¯•é›†å‰ {n_eval} è¡Œï¼ˆåŸå§‹ {len(X_test)} è¡Œï¼‰ã€‚")
		else:
			X_test_eval = X_test
			y_test_eval = y_test
			st.info(f"è¯„ä¼°è¡Œæ•°é™åˆ¶æœªå¯ç”¨ï¼Œä½¿ç”¨å…¨éƒ¨æµ‹è¯•é›† {len(X_test)} è¡Œã€‚")
		st.session_state["__eval_pack__"] = (task_type, X_test_eval, y_test_eval, artifacts)
